<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>基于NLP的关键词提取技术</title>
      <link href="/2021/11/24/nlp-text-key/"/>
      <url>/2021/11/24/nlp-text-key/</url>
      
        <content type="html"><![CDATA[<h2 id="关键词提取"><a href="#关键词提取" class="headerlink" title="关键词提取"></a>关键词提取</h2><p><strong>关键词</strong>是能够表达文档中心内容的词语，常用于计算机系统标引论文内容特征、信息检索、系统汇集以供读者检阅。关键词提取是文本挖掘领域的一个分支，是文本检索、文本比较、摘要生成、文档分类和聚类等文本挖掘研究的基础性工作。<br>从算法角度，关键词提取算法主要有两类：无监督关键词提取算法和有监督关键词提取算法。</p><ol><li>无监督关键词提取算法<br>不需要人工标注的语料，利用某些方法发现文本中比较重要的词作为关键词，进行关键词提取。该方法是先抽取出候选词，然后对每个候选词进行打分，输出topK个分值最高的候选词作为关键词。根据打分的策略不同，有不同的算法，如TF-IDF、TextRank、LDA等算法。<br>无监督关键词提取方法主要有三类：<strong>基于统计特征的关键词提取（TF-IDF）</strong>；<strong>基于词图模型的关键词提取（PageRank，TextRank）</strong>；<strong>基于主题模型的关键词提取（LDA）</strong>。</li></ol><p><img src="total_method.png" alt="无监督算法分类"></p><ul><li>基于统计特征的关键词提取算法思想是利用文档中词语的统计信息抽取文档的关键词；</li><li>基于词图模型的关键词提取首先要构建文档的语言网络图，然后对语言进行网络图分析，在这个图上寻找具有重要作用的词或者短语，这些短语就是文档的关键词；</li><li>基于主题关键词提取算法主要利用的是主题模型中关于主题分布的性质进行关键词提取；</li></ul><ol start="2"><li>有监督关键词提取算法<br>将关键词抽取过程视为二分类问题，先提取出候选词，然后对于每个候选词划定标签，要么是关键词，要么不是关键词，然后训练关键词抽取分类器。当新来一篇文档时，提取出所有的候选词，然后利用训练好的关键词提取分类器，对各个候选词进行分类，最终将标签为关键词的候选词作为关键词。</li></ol><p><img src="supervise.jpeg" alt="有监督算法处理流程"></p><ol start="3"><li><p>两种算法优缺点<br>无监督方法不需要人工标注训练集合的过程，因此更加快捷，但由于无法有效综合利用多信息对候选关键词排序，所以效果无法与有监督方法媲美；而有监督方法可以通过训练学习调节多种信息对于判断关键词的影响程度，因此效果更优，有监督的文本关键词提取算法需要高昂的人工成本（标注），因此现有的文本关键词提取主要采用适用性较强的无监督关键词提取。</p></li><li><p>关键词提取工具/sdk</p></li></ol><ul><li>jieba</li><li><a href="https://pypi.org/project/textrank4zh/0.3/" target="_blank" rel="noopener">Textrank4zh</a>（TextRank算法工具）</li><li><a href="https://pypi.org/project/snownlp/" target="_blank" rel="noopener">SnowNLP</a>（中文分析，简体中文文本处理）</li><li><a href="https://pypi.org/project/textblob/" target="_blank" rel="noopener">TextBlob</a>（英文分析）</li></ul><h2 id="TF-IDF关键词提取算法实现"><a href="#TF-IDF关键词提取算法实现" class="headerlink" title="TF-IDF关键词提取算法实现"></a>TF-IDF关键词提取算法实现</h2><p>TF-IDF算法的详细介绍及实现方法：<a href="https://blog.csdn.net/asialee_bird/article/details/81486700" target="_blank" rel="noopener">算法实现</a></p><h2 id="TextRank关键词提取算法实现"><a href="#TextRank关键词提取算法实现" class="headerlink" title="TextRank关键词提取算法实现"></a>TextRank关键词提取算法实现</h2><p>TextRank算法的详细介绍及实现方法：<a href="https://blog.csdn.net/asialee_bird/article/details/96894533" target="_blank" rel="noopener">算法实现</a></p><h2 id="LDA主题模型关键词提取算法实现"><a href="#LDA主题模型关键词提取算法实现" class="headerlink" title="LDA主题模型关键词提取算法实现"></a>LDA主题模型关键词提取算法实现</h2><ol><li>LDA（Latent Dirichlet Allocation）文档主题生成模型</li></ol><p><strong>主题模型</strong>是一种统计模型，用于发现文档集合中出现的抽象“主题”。主题建模是一种常用的文本挖掘工具，用于在文本体中发现隐藏的语义结构。<br>LDA也称三层贝叶斯概率模型，包含词、主题、文档三层结构；利用文档中单词的共现关系来对单词按主题聚类，得到“文档-主题”和“主题-单词”2个概率分布。<br>详细可以参考<a href="https://blog.csdn.net/v_JULY_v/article/details/41209515" target="_blank" rel="noopener">LDA通俗理解</a></p><ol start="2"><li>基于LDA主题模型的关键词提取算法实现</li></ol><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> gensim <span class="token keyword">import</span> corpora<span class="token punctuation">,</span> models<span class="token keyword">import</span> jieba<span class="token punctuation">.</span>posseg <span class="token keyword">as</span> jp<span class="token keyword">import</span> jieba<span class="token comment" spellcheck="true"># 简单文本处理</span><span class="token keyword">def</span> <span class="token function">get_text</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    flags <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'n'</span><span class="token punctuation">,</span> <span class="token string">'nr'</span><span class="token punctuation">,</span> <span class="token string">'ns'</span><span class="token punctuation">,</span> <span class="token string">'nt'</span><span class="token punctuation">,</span> <span class="token string">'eng'</span><span class="token punctuation">,</span> <span class="token string">'v'</span><span class="token punctuation">,</span> <span class="token string">'d'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 词性</span>    stopwords <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'的'</span><span class="token punctuation">,</span> <span class="token string">'就'</span><span class="token punctuation">,</span> <span class="token string">'是'</span><span class="token punctuation">,</span> <span class="token string">'用'</span><span class="token punctuation">,</span> <span class="token string">'还'</span><span class="token punctuation">,</span> <span class="token string">'在'</span><span class="token punctuation">,</span> <span class="token string">'上'</span><span class="token punctuation">,</span> <span class="token string">'作为'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 停用词</span>    words_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> text <span class="token keyword">in</span> texts<span class="token punctuation">:</span>        words <span class="token operator">=</span> <span class="token punctuation">[</span>w<span class="token punctuation">.</span>word <span class="token keyword">for</span> w <span class="token keyword">in</span> jp<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token keyword">if</span> w<span class="token punctuation">.</span>flag <span class="token keyword">in</span> flags <span class="token operator">and</span> w<span class="token punctuation">.</span>word <span class="token operator">not</span> <span class="token keyword">in</span> stopwords<span class="token punctuation">]</span>        words_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>words<span class="token punctuation">)</span>    <span class="token keyword">return</span> words_list<span class="token comment" spellcheck="true"># 生成LDA模型</span><span class="token keyword">def</span> <span class="token function">LDA_model</span><span class="token punctuation">(</span>words_list<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 构造词典</span>    <span class="token comment" spellcheck="true"># Dictionary()方法遍历所有的文本，为每个不重复的单词分配一个单独的整数ID，同时收集该单词出现次数以及相关的统计信息</span>    dictionary <span class="token operator">=</span> corpora<span class="token punctuation">.</span>Dictionary<span class="token punctuation">(</span>words_list<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'打印查看每个单词的id:'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>dictionary<span class="token punctuation">.</span>token2id<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 打印查看每个单词的id</span>    <span class="token comment" spellcheck="true"># 将dictionary转化为一个词袋</span>    <span class="token comment" spellcheck="true"># doc2bow()方法将dictionary转化为一个词袋。得到的结果corpus是一个向量的列表，向量的个数就是文档数。</span>    <span class="token comment" spellcheck="true"># 在每个文档向量中都包含一系列元组,元组的形式是（单词 ID，词频）</span>    corpus <span class="token operator">=</span> <span class="token punctuation">[</span>dictionary<span class="token punctuation">.</span>doc2bow<span class="token punctuation">(</span>words<span class="token punctuation">)</span> <span class="token keyword">for</span> words <span class="token keyword">in</span> words_list<span class="token punctuation">]</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'输出每个文档的向量:'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>corpus<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 输出每个文档的向量</span>    <span class="token comment" spellcheck="true"># LDA主题模型</span>    <span class="token comment" spellcheck="true"># num_topics -- 必须，要生成的主题个数。</span>    <span class="token comment" spellcheck="true"># id2word    -- 必须，LdaModel类要求我们之前的dictionary把id都映射成为字符串。</span>    <span class="token comment" spellcheck="true"># passes     -- 可选，模型遍历语料库的次数。遍历的次数越多，模型越精确。但是对于非常大的语料库，遍历太多次会花费很长的时间。</span>    lda_model <span class="token operator">=</span> models<span class="token punctuation">.</span>ldamodel<span class="token punctuation">.</span>LdaModel<span class="token punctuation">(</span>corpus<span class="token operator">=</span>corpus<span class="token punctuation">,</span> num_topics<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> id2word<span class="token operator">=</span>dictionary<span class="token punctuation">,</span> passes<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> lda_model<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    texts <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'作为千元机中为数不多拥有真全面屏的手机，OPPO K3一经推出，就簇拥不少粉丝'</span><span class="token punctuation">,</span> \             <span class="token string">'很多人在冲着这块屏幕购买了OPPO K3之后，发现原来K3的过人之处不止是在屏幕上'</span><span class="token punctuation">,</span> \             <span class="token string">'OPPO K3的消费者对这部手机总体还是十分满意的'</span><span class="token punctuation">,</span> \             <span class="token string">'吉利博越PRO在7月3日全新吉客智能生态系统GKUI19发布会上正式亮相'</span><span class="token punctuation">,</span> \             <span class="token string">'今年上海车展，长安CS75 PLUS首次亮相'</span><span class="token punctuation">,</span> \             <span class="token string">'普通版车型采用的是双边共双出式排气布局；运动版本车型采用双边共四出的排气布局'</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># 获取分词后的文本列表</span>    words_list <span class="token operator">=</span> get_text<span class="token punctuation">(</span>texts<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'分词后的文本：'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>words_list<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 获取训练后的LDA模型</span>    lda_model <span class="token operator">=</span> LDA_model<span class="token punctuation">(</span>words_list<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 可以用 print_topic 和 print_topics 方法来查看主题</span>    <span class="token comment" spellcheck="true"># 打印所有主题，每个主题显示5个词</span>    topic_words <span class="token operator">=</span> lda_model<span class="token punctuation">.</span>print_topics<span class="token punctuation">(</span>num_topics<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> num_words<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'打印所有主题，每个主题显示5个词:'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>topic_words<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 输出该主题的的词及其词的权重</span>    words_list <span class="token operator">=</span> lda_model<span class="token punctuation">.</span>show_topic<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'输出该主题的的词及其词的权重:'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>words_list<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可在本机python环境中运行并查看结果。</p><h2 id="Word2Vec词聚类关键词提取算法实现"><a href="#Word2Vec词聚类关键词提取算法实现" class="headerlink" title="Word2Vec词聚类关键词提取算法实现"></a>Word2Vec词聚类关键词提取算法实现</h2><ol><li><p>Word2Vec词向量表示<br>利用浅层神经网络模型自动学习词语在语料库中的出现情况，把词语嵌入到一个高维的空间中，通常在100-500维，在高维空间中词语被表示为词向量的形式。<br>特征词向量的抽取是基于已经训练好的词向量模型。</p></li><li><p>K-means聚类算法<br>聚类算法旨在数据中发现数据对象之间的关系，将数据进行分组，使得组内的相似性尽可能的大，组间的相似性尽可能的小。</p></li></ol><p><strong>算法思想是</strong>：首先随机选择K个点作为初始质心，K为用户指定的所期望的簇的个数，通过计算每个点到各个质心的距离，将每个点指派到最近的质心形成K个簇，然后根据指派到簇的点重新计算每个簇的质心，重复指派和更新质心的操作，直到簇不发生变化或达到最大的迭代次数则停止。</p><ol start="3"><li>基于Word2Vec词聚类关键词提取方法的实现过程</li></ol><p><strong>主要思路</strong>是对于用词向量表示的词语，通过K-means算法对文章中的词进行聚类，选择聚类中心作为文本的一个主要关键词，计算其他词与聚类中心的距离即相似度，选择topK个距离聚类中心最近的词作为关键词，而这个词间相似度可用Word2Vec生成的向量计算得到。</p><p><strong>具体步骤如下：</strong></p><ul><li>对语料进行Word2Vec模型训练，得到词向量文件；</li><li>对文本进行预处理获得N个候选关键词；</li><li>遍历候选关键词，从词向量文件中提取候选关键词的词向量表示；</li><li>对候选关键词进行K-means聚类，得到各个类别的聚类中心（需要人为给定聚类的个数）；</li><li>计算各类别下，组内词语与聚类中心的距离（欧几里得距离或曼哈顿距离），按聚类大小进行降序排序；</li><li>对候选关键词计算结果得到排名前TopK个词语作为文本关键词。</li></ul><p>注：第三方工具包Scikit-learn提供了K-means聚类算法的相关函数，本文用到了sklearn.cluster.KMeans()函数执行K-means算法，sklearn.decomposition.PCA()函数用于数据降维以便绘制图形。</p><h2 id="信息增益关键词提取算法实现"><a href="#信息增益关键词提取算法实现" class="headerlink" title="信息增益关键词提取算法实现"></a>信息增益关键词提取算法实现</h2><p>信息增益算法的详细介绍及实现方法：<a href="https://blog.csdn.net/asialee_bird/article/details/81084783" target="_blank" rel="noopener">算法实现</a></p><h2 id="互信息关键词提取算法实现"><a href="#互信息关键词提取算法实现" class="headerlink" title="互信息关键词提取算法实现"></a>互信息关键词提取算法实现</h2><ol><li><p>互信息（Mutual Information，MI）<br>在概率论和信息论中，两个随机变量的互信息或转移信息（transinformation）是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布 $p(X,Y)$ 和分解的边缘分布的乘积 $p(X)p(Y)$ 的相似程度。互信息是度量两个事件集合之间的相关性（mutual dependence）。<br>互信息被广泛用于度量一些语言现象的相关性。在信息论中，互信息被用于衡量两个词的相关度，也用来计算词与类别之间的相关性。</p></li><li><p>互信息计算公式<br>其中 $p(X,Y)$ 是X和Y的联合概率分布函数，而p(X)和p(Y)分别是X和Y的边缘概率分布函数。<br>$$<br>I(X; Y) = \sum_{y\in Y}\sum_{x\in X}p(x, y) \log(\frac{p(x,y)}{p(x)p(y)})<br>$$</p></li></ol><p>$$<br>\begin{aligned}<br>I(X; Y) &amp;= \sum_{x,y}p(x, y) \log{\frac{p(x, y)}{p(x)p(y)}}\\<br>&amp;= \sum_{x,y}p(x, y) \log{\frac{p(x, y)}{p(x)}} - \sum_{x,y}p(x, y) \log p(y)\\<br>&amp;= \sum_{x,y}p(x)p(y|x) \log(py|x) - \sum_{x,y}p(x, y) \log{p(y)}\\<br>&amp;= \sum_xp(x)(\sum_yp(y|x) \log{p(y|x)}) - \sum_y \log{p(y)}(\sum_xp(x,y))\\<br>&amp;= -\sum_xp(x)H(Y|X=x) - \sum_y \log{p(y)p(y)}\\<br>&amp;= -H(Y|X) + H(Y)\\<br>&amp;= H(Y) - H(Y|X)<br>\end{aligned}<br>$$</p><ol start="3"><li>互信息算法实现</li></ol><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment" spellcheck="true"># 训练集和训练标签</span>x_train <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>y_train <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 测试集和测试标签</span>x_test <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>x_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 转为array</span><span class="token comment" spellcheck="true"># 存储每个特征与标签相关性得分</span>features_score_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>x_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 计算每个特征与标签的互信息</span>    feature_info <span class="token operator">=</span> metrics<span class="token punctuation">.</span>mutual_info_score<span class="token punctuation">(</span>y_train<span class="token punctuation">,</span> x_train<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">)</span>    features_score_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>feature_info<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>features_score_list<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>请在本机查看运行结果。</p><ol start="4"><li>信息论中的互信息和决策树中的信息增益的关系</li></ol><p>$$<br>\begin{aligned}<br>&amp; 信息论中互信息：\\<br>&amp; I(X;Y) = H(X) - H(X|Y)\\<br>&amp; 决策树中信息增益：\\<br>&amp; G(D,A) = H(D) - H(D|A)<br>\end{aligned}<br>$$</p><p>两者表达意思是一样的，都是表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。</p><p>注：</p><ul><li>标准化互信息（Normalized Mutual Information，NMI）可以用来衡量两种聚类结果的相似度。</li><li>标准化互信息Sklearn实现：metrics.normalized_mutual_info_score(y_train, x_train[:,i])。</li><li>点互信息（Pointwise Mutual Information，PMI）这个指标来衡量两个事物之间的相关性（比如两个词）。</li></ul><h2 id="卡方检验关键词提取算法实现"><a href="#卡方检验关键词提取算法实现" class="headerlink" title="卡方检验关键词提取算法实现"></a>卡方检验关键词提取算法实现</h2><ol><li><p>卡方检验<br>卡方是数理统计中用于检验两个变量独立性的方法，是一种确定两个分类变量之间是否存在相关性的统计方法，经典的卡方检验师检验定性自变量对定性因变量的相关性。</p></li><li><p>基本思路</p></li></ol><ul><li>原假设：两个变量是独立的</li><li>计算实际观察值和理论值之间的偏离程度</li><li>如果偏差足够小，小于设定阈值，就接受原假设；否则就否定原假设，认为两变量是相关的。</li></ul><ol start="3"><li>计算公式</li></ol><p>$$<br>x^2 = \sum \frac{(A-T)^2}{T}<br>$$</p><p>其中，A为实际值，T为理论值。卡方检验可用于文本分类问题中的特征选择，此时不需要设定阈值，只关心找到最为相关的topK个特征。基本思想：比较理论频数和实际频数的吻合程度或者拟合优度问题。</p><ol start="4"><li>基于sklearn的卡方检验实现</li></ol><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> SelectKBest<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> chi2<span class="token comment" spellcheck="true"># 训练集和训练标签</span>x_train <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>y_train <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 测试集和测试标签</span>x_test <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>y_test <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 卡方检验选择特征</span>chi2_model <span class="token operator">=</span> SelectKBest<span class="token punctuation">(</span>chi2<span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 选择k个最佳特征</span><span class="token comment" spellcheck="true"># 该函数选择训练集里的k个特征，并将训练集转化所选特征</span>x_train_chi2 <span class="token operator">=</span> chi2_model<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将测试集转化为所选特征</span>x_test_chi2 <span class="token operator">=</span> chi2_model<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'各个特征的得分：'</span><span class="token punctuation">,</span> chi2_model<span class="token punctuation">.</span>scores_<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'各个特征的p值：'</span><span class="token punctuation">,</span> chi2_model<span class="token punctuation">.</span>pvalues_<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># p值越小，置信度越高，得分越高</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'所选特征的索引：'</span><span class="token punctuation">,</span> chi2_model<span class="token punctuation">.</span>get_support<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'特征提取转换后的训练集和测试集...'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'x_train_chi2:'</span><span class="token punctuation">,</span> x_train_chi2<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'x_test_chi2:'</span><span class="token punctuation">,</span> x_test_chi2<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>请在本机查看运行结果。</p><h2 id="基于树模型关键词提取算法实现"><a href="#基于树模型关键词提取算法实现" class="headerlink" title="基于树模型关键词提取算法实现"></a>基于树模型关键词提取算法实现</h2><ol><li>树模型<br>主要包括决策树和随机森林，基于树的预测模型（sklearn.tree模块和sklearn.ensemble模块）能够用来计算特征的重要程度，因此能用来去除不相关的特征（结合sklearn.feature_selection.SelectFromModel）</li></ol><p>sklearn.ensemble模块包含了两种基于随机决策树的平均算法：<strong>RandomForest</strong>算法和<strong>Extra-Trees</strong>算法。这两种算法都采用了很流行的树设计思想：perturb-and-combine思想。这种方法会在分类器的构建时，通过引入随机化，创建一组各不一样的分类器。这种ensemble方法的预测会给出各个分类器预测的平均。</p><ul><li><p>RandomForests在随机森林(RF)中，该ensemble方法中的每棵树都基于一个通过可放回抽样(boostrap)得到的训练集构建。相反的，在features的子集中随机进行split反倒是最好的split方式。sklearn的随机森林(RF)实现通过对各分类结果预测求平均得到，而非让每个分类器进行投票（vote）。</p></li><li><p>Ext-Trees在Ext-Trees中（详见ExtraTreesClassifier和ExtraTreesRegressor），该方法中，随机性在划分时会更进一步进行计算。在随机森林中，会使用候选feature的一个随机子集，而非查找最好的阈值，对于每个候选feature来说，阈值是抽取的，选择这种随机生成阈值的方式作为划分原则。</p></li></ul><ol start="2"><li>树模型的关键词提取算法实现</li></ol><ul><li>部分代码实现</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>tree <span class="token keyword">import</span> DecisionTreeClassifier<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> RandomForestClassifier<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> ExtraTreesClassifier<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> SelectFromModel<span class="token comment" spellcheck="true"># 导入SelectFromModel结合ExtraTreesClassifier计算特征重要性，并按重要性阈值选择特征。</span><span class="token comment" spellcheck="true"># 基于树模型进行模型选择</span>clf_model <span class="token operator">=</span> ExtraTreesClassifier<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span><span class="token number">250</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>clf_model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 获取每个词的特征权重,数值越高特征越重要</span>importances <span class="token operator">=</span> clf_model<span class="token punctuation">.</span>feature_importances_<span class="token comment" spellcheck="true"># 选择特征重要性为1.5倍均值的特征</span>model <span class="token operator">=</span> SelectFromModel<span class="token punctuation">(</span>clf_model<span class="token punctuation">,</span> threshold<span class="token operator">=</span><span class="token string">'1.5*mean'</span><span class="token punctuation">,</span> prefit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>x_train_new <span class="token operator">=</span> model<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 返回训练集所选特征</span>x_test_new <span class="token operator">=</span> model<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 返回测试集所选特征</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>部分代码实现</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 训练集和训练标签</span>x_train<span class="token punctuation">,</span> y_train<span class="token comment" spellcheck="true"># 候选特征词列表</span>words_list<span class="token comment" spellcheck="true"># 基于树模型进行模型选择</span>forest <span class="token operator">=</span> RandomForestClassifier<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span><span class="token number">250</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>forest<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>importances <span class="token operator">=</span> forest<span class="token punctuation">.</span>feature_importances_  <span class="token comment" spellcheck="true"># 获取每个词的特征权重</span><span class="token comment" spellcheck="true"># 将词和词的权重存入字典</span>feature_words_dic <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>words_list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    feature_words_dic<span class="token punctuation">[</span>words_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> importances<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 对字典按权重由大到小进行排序</span>words_info_dic_sort <span class="token operator">=</span> sorted<span class="token punctuation">(</span>words_info_dic<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将关键词和词的权重分别存入列表</span>keywords_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 关键词列表</span>features_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 关键权重列表</span><span class="token keyword">for</span> word <span class="token keyword">in</span> words_info_dic_sort<span class="token punctuation">:</span>    keywords_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>word<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    features_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>word<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 选取前一千个关键词和权重写入文本文件</span>keywords <span class="token operator">=</span> keywords_list<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">1000</span><span class="token punctuation">]</span>features <span class="token operator">=</span> features_list<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 将含有关键字的文本写入文件</span><span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'data/keywords_features.txt'</span><span class="token punctuation">,</span> <span class="token string">'a'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>keywords<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>keywords<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">'\t'</span> <span class="token operator">+</span> features<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文总结了本人在实验过程中所用到的常用关键词抽取方法，实验数据是基于公司的内部数据，但此篇总结只是方法上的讲解和实现，没有针对某一具体数据集做相应的结果分析。从实验中可以很明显看出有监督关键词抽取方法通常会显著好于无监督方法，但是有监督方法依赖一定规模的标注数据。</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>广义文本的向量表示方法</title>
      <link href="/2021/11/12/text2vec/"/>
      <url>/2021/11/12/text2vec/</url>
      
        <content type="html"><![CDATA[<p>NLP的基础工作就是文本处理，向量化文本表示的核心目的是将文本表示为一系列的能够表达文本语义的向量，有word2vec、str2vec和doc2vec技术，区别是目标处理单元不同。<br>word2vec即词向量，str2vec为句向量，doc2vec则为文档向量。</p><h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>词袋（bag of word）模型是最早的以词为基础处理单元的文本向量化方法。该模型产生的向量与原来文本中单词出现的顺序没有关系，而是词典中每个单词再文本中出现的频率。该方法虽然简单，但有以下问题：</p><ol><li>维度灾难</li><li>无法保留词序信息</li><li>存在语义鸿沟</li></ol><p>随着互联网技术的发展，大量无标注数据的产生，研究中心转移到利用无标注数据挖掘有价值的信息上来。word2vec（词向量）就是为了利用神经网络，从大量无标注的文本中提取有用的信息而产生。</p><p>词袋模型只是将词语符号化，所以词袋模型是不包含语义信息的。如何使词表示包含语义信息是该领域研究者面临的问题。分布假设（distributional hypothesis）的提出为解决上述问题提供了理论基础。该假设的核心思想是：上下文相似的词，其语义也相似。随后大量研究人员整理了利用上下文表示词义的方法，这类方法就是有名的词空间模型（word space model）。通过语言模型构建上下文与目标词之间的关系，是一种常见的方法，神经网络词向量模型就是根据上下文与目标词之间的关系进行建模。</p><h2 id="NNLM"><a href="#NNLM" class="headerlink" title="NNLM"></a>NNLM</h2><p>神经网络语言模型（Neural Network Language Model，NNLM）与传统方法估算的不同在于通过一个神经网络结构对n元条件概率进行估计。由于NNLM模型使用低维紧凑的词向量对上下文进行表示，解决了词袋模型带来的数据稀疏、语义鸿沟等问题。另一方面，在相似的上下文环境中，NNLM模型可以预测出相似的目标词，而传统模型无法做到这一点。例如，如果在预料中 A=“小狗在院子里趴着” 出现1000次， B=“小猫在院子里趴着” 出现1次。A和B的唯一区别就是狗和猫，两个词无论在语义还是语法上都相似。根据频率来估算概率P(A) &gt;&gt; P(B)，这显然不合理。如果采用NNLM计算P(A)~P(B)，因为NNLM模型采用低维的向量表示词语，假定相似的词，则其词向量也相似。</p><p><img src="str_nnlm.png" alt="NNLM词向量处理模型"></p><h2 id="C-amp-W"><a href="#C-amp-W" class="headerlink" title="C&amp;W"></a>C&amp;W</h2><p>C&amp;W = context &amp; word(上下文和目标词)，主要目的并不在于生成一份好的词向量，甚至不想训练语言模型，而是要用这份词向量去完成NLP里面的各项任务，比如词性标注、命名实体识别、短语识别、语义角色标注等等。NNLM模型的目标是构建一个语言概率模型，而C&amp;W则是以生成词向量为目标的模型。C&amp;W模型并没有采用语言模型去求解词语上下文的条件概率，而是直接对n元短语打分，这就省去了NNLM模型中从隐藏层到输出层的权重计算，大大降低了运算量。其核心机理是：如果n元短语在语料库中出现过，那么模型会给该短语打高分；如果是未出现则在语料库中的短语则会得到较低的评分。</p><h2 id="CBOW-and-Skip-gram"><a href="#CBOW-and-Skip-gram" class="headerlink" title="CBOW and Skip-gram"></a>CBOW and Skip-gram</h2><p>为了更高效的获取词向量，研究者在NNLM和C&amp;W模型的基础上保留其核心部分，得到CBOW（Continuous Bag of Words）模型和Skip-gram模型。</p><p><img src="cbow.png" alt="CBOW模型"></p><p>模型使用一段文本的中间词作为目标词，去掉了隐藏层，大幅度提升了计算频率。此外，CBOW模型还使用上下文歌词的词向量的平均值替代NNLM模型各个拼接的词向量。即根据上下文来预测当前词语的频率，且上下文所有词出现频率的影响权重是一样的。Skip-gram模型同样没有隐藏层，与CBOW模型输入上下文词的平均词向量不同，它是从目标词W的上下文中选择一个词，将其词向量组成上下文的表示。即根据当前词语来预测上下文概率。</p><h2 id="doc2vec-str2vec"><a href="#doc2vec-str2vec" class="headerlink" title="doc2vec/str2vec"></a>doc2vec/str2vec</h2><p>利用word2vec计算词语间的相似度有非常好的效果，word2vec技术也可以用于计算句子或者其他长文本间的相似度，其一般做法是对文本粉刺后，提取其关键词，用词向量表示这些关键词，接着对关键词向量求平均或者将其拼接，最后利用词向量计算文本间的相似度。这种方法丢失了文本中的次序信息，而文本的语序包含重要信息。为此，有研究者在word2vec的基础上提出了文档向量化（doc2vec），又称str2vec和para2vec。</p><p>doc2vec技术存在两种模型–Distributed Memory(DM)和Distributed Bag of Words(DBOW)，分别对应word2vec技术里的CBOW和Skip-gram模型。Doc2vec相对于word2vec不同之处在于，在输入层，增添了一个新句子向量Paragraph vector， Paragraph vector可以被看作是另一个词向量，它扮演了一个记忆，词袋模型中，因为每次训练只会截取句子中一小部分词训练，而忽略了除了本次训练词以外该句子中的其他词，忽略了文本的词序问题。而doc2vec中的paragraph vector则弥补了这方面的不足，它每次训练也是滑动截取句子中一小部分词来训练，paragraph vector在同一个句子的若干次训练中是共享的，所以同一句话会有多次训练，每次训练中输入都包含paragraph vector。它可以被看作是句子的主旨（中心思想），有了它，该句子的主旨每次都会被放入作为输入的一部分来训练。这样每次训练中，不光是训练了词，得到了词向量。同时随着一句话每次滑动截取若干词训练的过程中，作为每次训练的输入层一部分的共享paragraph vector，该向量的主旨会越来越准确。</p><p><img src="mod_dbow.png" alt="DBOW模型示意图"></p><p>那么doc2vec是怎么预测新的句子paragraph vector呢？其实在预测新句子的时候，还是会将该paragraph vector随机初始化，放入模型中再重新根据随机梯度下降不断迭代求得最终稳定下来的句子向量。不过在预测过程中，模型里的词向量还有投影层到输出层的softmax weights参数是不会变的，这样在不断迭代中只会更新paragraph vector，其他参数均已固定，只需很少的时间就能计算出带预测的paragraph vector。</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 内容推荐 </tag>
            
            <tag> 文本向量化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于广义内容的推荐算法系统</title>
      <link href="/2021/11/04/context-recalg/"/>
      <url>/2021/11/04/context-recalg/</url>
      
        <content type="html"><![CDATA[<h1 id="写在前面-什么是基于内容推荐算法"><a href="#写在前面-什么是基于内容推荐算法" class="headerlink" title="写在前面 - 什么是基于内容推荐算法"></a>写在前面 - 什么是基于内容推荐算法</h1><p>广义内容，是指包括文本、图片、音频、视频等在内的多媒体信息。<br>基于内容的推荐算法(Content-Based Recommendations)是基于标的物相关信息、用户相关信息及用户对标的物的操作行为来构建推荐算法模型，为用户提供推荐服务。这里的标的物相关信息可以是对标的物文字描述的metadata信息、标签、用户评论、人工标注的信息等。用户相关信息是指人口统计学信息(如年龄、性别、偏好、地域、收入等等)。用户对标的物的操作行为可以是评论、收藏、点赞、观看、浏览、点击、加购物车、购买等。基于内容的推荐算法一般只依赖于用户自身的行为为用户提供推荐，不涉及到其他用户的行为。</p><p>广义的标的物相关信息不限于文本信息，图片、语音、视频等都可以作为内容推荐的信息来源，只不过这类信息处理成本较大，不光是算法难度大、处理的时间及存储成本也相对更高。</p><p>基于内容的推荐算法算是最早应用于工程实践的推荐算法，有大量的应用案例，如今日头条的推荐有很大比例是基于内容的推荐算法。</p><h1 id="基于内容的推荐算法实现原理"><a href="#基于内容的推荐算法实现原理" class="headerlink" title="基于内容的推荐算法实现原理"></a>基于内容的推荐算法实现原理</h1><p>基于内容的推荐算法的基本原理是根据用户的历史行为，获得用户的兴趣偏好，为用户推荐跟他的兴趣偏好相似的标的物，下图展示了基于内容的推荐算法逻辑过程。<br><img src="alg_resis.jpeg" alt="算法逻辑"></p><p>从上图也可以看出，要做基于内容的个性化推荐，一般需要三个步骤，它们分别是：基于用户信息及用户操作行为构建用户特征表示、基于标的物信息构建标的物特征表示、基于用户及标的物特征表示为用户推荐标的物。<br><img src="alg_struc.jpeg" alt="核心步骤"></p><p>本节我们先介绍如何基于核心步骤的1和2喂用户做推荐（即达到步骤3），然后分别对这三个步骤加以说明，介绍每个步骤都有哪些方法和策略可供选择。</p><h2 id="基于用户和标的物特征为用户推荐的核心思想"><a href="#基于用户和标的物特征为用户推荐的核心思想" class="headerlink" title="基于用户和标的物特征为用户推荐的核心思想"></a>基于用户和标的物特征为用户推荐的核心思想</h2><p>有了用户特征和标的物特征，推荐思路总结有三个：</p><p><strong>1.基于用户历史行为记录了做推荐</strong><br>我们需要事先计算标的物之间的相似性，然后将用户历史记录中的标的物的相似标的物推荐给用户。<br>不管标的物包含哪类信息，一般的思路是将标的物特征转化为向量化表示，有了向量化表示，就可以通过cosin余弦计算两个标的物向量之间的相似度，余弦是相邻边比值，当比值近似1的情况，相似度最好。</p><p><strong>2.用户和标的物特征都用显式的标签表示，利用该表示做推荐</strong><br>标的物用标签来表示，反过来，每个标签就可以关联一组标的物，那么根据用户的标签表示，用户的兴趣标签就可以关联到一组标的物，这组通过标签关联到的标的物，就可以作为给用户推荐的候选集合。这类方法就是所谓的倒排索引，是搜索业务通用的解决方案。</p><p><strong>3.用户和标的物嵌入到同一个向量空间，基于向量相似（即计算向量余弦值）做推荐</strong><br>当用户和标的物嵌入到同一个向量空间后，就可以通过计算用户和标的物之间的相似度，然后按照标的物跟用户的相似度，为用户推荐相似度高的标的物。还可以基于用户向量表示计算用户相似度，将相似用户喜欢的标的物推荐给该用户，这时标的物的嵌入是不必要的。</p><p>上面就是基于内容推荐的核心思想，那么如何构建用户表示特征向量和标的物表示特征向量就是推荐系统的关键。</p><h2 id="构建用户特征向量"><a href="#构建用户特征向量" class="headerlink" title="构建用户特征向量"></a>构建用户特征向量</h2><p>用户的特征总结来讲有如下方法构建</p><ul><li>基于用户对标的物的操作行为，如点击/购买/收藏/播放/评论等构建用户对标的物的偏好画像；</li><li>基于用户自身的人口统计学特征表示。</li></ul><p>具体来说：<br><strong>1.用户行为记录作为显示特征</strong><br>比如用户浏览过A、B、C三个视频，同时根据每个视频用户观看时长占分别视频总时长的比例给用户行为打分，这是用户的兴趣偏好就可以记录为{$(A, S1), (B, S2), (C, S3)$}，其中S1, S2, S3分别是用户对视频A、B、C的评分。</p><p>该方案直接将用户历史操作过的标的物作为用户特征表示，在推荐时可以将与用户操作过的标的物相似的标的物推荐给用高糊。</p><p><strong>2.显示的标签特征</strong><br>如果标的物是有标签来描述，那么这些标签可以用来表征标的物。用户的兴趣画像也可以基于用户对标的物的行为来打上对应的标签。拿视频推荐来举例，如果用户过去看了科幻和恐怖类电影，那么恐怖和科幻就是用户的偏好标签了。<br>每个标的物的标签可以是包含权重的，而用户对标的物的操作行为也是有权重的，而用户对标的物的操作行为也是有权重的，从而用户的兴趣标签是有权重的。<br>在具体推荐时，可以将用户的兴趣标签关联到的标的物(具备该标签的标的物)推荐给用户。</p><p><strong>3.向量方法的兴趣特征</strong><br>可以基于标的物的信息将标的物嵌入到向量空间中，利用向量来表示标的物，稍后讲解嵌入的算法实现方案。有了标的物的向量化表示，用户的兴趣向量就可以用他操作过的标的物的向量的平均向量来表示了。<br>这里表示用户兴趣向量有多种策略，可以基于用户对操作过的标的物的评分以及时间加权来获取用户的加权偏好向量，而不是直接取平均。另外，我们也可以根据用户操作过的标的物之间的相似度，喂用户构建多个兴趣向量（比如对标的物聚类，用户在某一类上操作过的标的物的向量均值作为用户在这个类别上的兴趣向量），从而更好的表达用户多方位的兴趣偏好。<br>有了用户的兴趣向量及标的物的兴趣向量，可以基于向量相似性计算用户对标的物的偏好度，再基于偏好度大小来为用户推荐标的物。</p><p><strong>4.通过交互方式获取用户兴趣标签</strong><br>在用户首次注册让用户选择自己的兴趣偏好标签，一旦用户勾选了自己的兴趣标签，那么这些兴趣标签就是系统为用户推荐标的物的逻辑基础，具体推荐策略逻辑就可以按照上述方法进行。</p><p><strong>5.用户的人口统计学特征</strong><br>用户在登陆、注册时提供的关于自身相关的信息、通过运营活动用户填写的信息、通过用户行为利用算法逻辑得出的结论，如年龄、性别、地域、收入、爱好、居住地、工作地点等是非常主要的信息。基于这些关于用户维度的信息，我们可以将用户特征用向量化表示出来，向量的维度就是可获取的用户特征数。<br>有了用户特征向量就可以计算用户相似度，将相思用户喜欢的标的物推荐给用户。</p><h2 id="构建标的物特征表示"><a href="#构建标的物特征表示" class="headerlink" title="构建标的物特征表示"></a>构建标的物特征表示</h2><p>标的物的特征，一般可以利用显示的标签来表示，也可以利用隐式的向量（当然one-hot编码也是向量表示，但是不是隐式的）来刻画，向量的每个维度就是一个隐式的特征项。前面提到某些推荐算法需要计算标的物之间的相似度，下面我们在讲标的物的各种特征表示时，也简单介绍一下标的物之间的相似度计算方法。顺便说一下，标的物关联标的物的推荐方式也需要知道标的物之间的相似度。下面从四个方面来详细讲解怎么构建标的物的特征表示。</p><p><strong>1.标的物包含标签信息</strong><br>最简单的方式是将标签按照某种顺序排列，每个标签看成一个纬度，那么每个标的物就可以表示成一个N维向量，如 {item， [tag1, tag2, tag3, …, tagN]}，如果标的物包含某个标签，向量在相应标签上的分量值为1，否则为0，即所谓的one-hot编码。通常N可能非常大，比如视频领域，N可能是几万或者几十万上百万，这时向量是稀疏矩阵，因为一般标的物只有几个或者几十个标签，采用稀疏向量的表示来优化向量存储和计算，提升效率。有个标的物基于标签的向量化表示，很容易基于cosin余弦计算相似度了。</p><p>实际上标签不是这么简单的，有很多业务标签是分级的，比如电商（像淘宝），有多级标签，标签的层级关系形成一颗树状结构，这时向量化有个简单的表示方案，即只考虑最低层级标签（叶节点），基于叶节点标签构建向量表示。更复杂的方法，可以基于层级结构构建标签表示及计算标的物相似度。<br><img src="tag_tree.png" alt="标签树"></p><p>标签是可以通过算法获取的，比如通过NLP从文本信息中提取关键词作为标签。对于图片/视频，通过他们的描述信息提取标签，另外可以通过目标检测的方法从图片/视频中提取相关对象构建标签。</p><p>标签可以是用户打的，很多产品在用户与标的物交互时互相可以为标的物打标签，这些标签就是标的物的一种刻画。标签也可以是人工标注，Netflix在做推荐时，就请了上万专家对视频从上千个纬度来打标签，质量很高。很多行业的标的物来源于第三方供应商，他们在入驻平台时会被要求按照某些规范填写相关标签信息。</p><p><strong>2.标的物具备结构化的信息</strong><br>有些行业标的物是具备结构化信息的，如视频行业，一般会有媒体资料库，库中对每个节目都会有标题、演职员、导演、标签、评分、地域等维度数据，这类数据一般存在传统数据库中。这类数据，可以将一个字段（也是一个特征）作为向量的一个纬度，这时向量化表示每个维度的值不一定是数值，但是形式还是向量化的形式，即所谓的向量空间模型（Vector Space Model，简称VSM）。这时可以通过如下的方式计算两个标的物之间的相似度。<br>假设两个标的物的向量表示分别为<br>$V1=(p_1, p_2, p_3, ……, p_k)$<br>$V2=(q_1, q_2, q_3, ……, q_k)$</p><p>这时两个标的物的相似性计算公式为 $$sim(V_1, V_2)=\sum_{t=1}^k ksim(p_t, q_t)$$</p><p>其中代表的是向量的两个分量之间的相似度。可以采用Jacard相似度等各种方法计算两个分量之间的相似度。上面公式中还可以针对不同的分量采用不同的权重策略，见下面公式，其中第t个分量（特征）的权重，具体权重的数值可以根据对业务的理解来人工设置，或者利用机器学习算法来训练学习得到。<br>$$sim(V_1, V_2)=\sum_{t=1}^k w_t*sim(p_t, q_t)$$</p><p><strong>3.包含文本信息的标的物的特征表示</strong><br>像头条等新闻资讯或搜索类app，标的物就是一篇篇文章（其中会包含图片或视频），文本信息是最重要的信息形式，构建标的物之间的相似性有很多种方法。下面对常用的方法做一些讲解说明。</p><p>a. 利用TF-IDF将文本信息转化为特征向量<br>TF-IDF通过将所有文档（即标的物）粉刺，获得所有不同词的集合（假设有M个词），那么就可以为每个文档构建一个M维（每个词就是一个维度向量）的向量，而该向量中某个词所在维度的值可以通过统计每个词在文档中的重要性来衡量，这个重要性的度量就是TF-IDF。<br>TF是某个词在某篇文章中出现的频次，用于衡量这个词在文档中的重要性，出现次数越多的词重要性越高，副词（的，地）和语气助词等会去掉度量，这些词对构建向量是没有任何实际价值的。TF具体计算公式如下<br>$$<br>TF(t_k, d_j)=\frac{||t_k\in d_j||}{||d_j||}<br>$$<br>其中分子tk是第k个词在文档中出现的次数，分母是dj中词的总个数。</p><p>IDF代表的是某个词在所有文档中的“区分度”，如果某个词只在少量几个文档中出现，那么它包含的价值就是巨大的，如果某个词在很多文档中出现，那么它就不能很好的度量这个文档。下面是IDF的计算公式，其中N是所有文档的个数，是包含次的文档个数，这个公式与前面描述一致，稀有词的区分度大。<br>$$<br>IDF(t_k)=\log \frac{N}{n_k}<br>$$<br>有了上面对TF和IDF的定义，实际的TF-IDF就是上面两个量的乘积：<br>$$<br>TF-IDF(t_k, d_j) = TF(t_k, d_j) * IDF(t_k)<br>$$<br>有了基于TF-IDF计算的标的物（即文档表示为TF-IDF向量）的向量表示，就很容易计算两个标的物（文档）的相似度了（cosin余弦相似度）</p><p>b. 利用LDA算法构建文档（标的物）的主题<br>LDA算法是一类文档主题生成模型，包含词、主题、文档三层结构，是一个三层的贝叶斯概率模型。对于语料库中的每篇文档，LDA定义了如下生成过程：</p><ol><li>对于每篇文档，从主题分布中抽取一个主题；</li><li>从上述被抽取到的主题所对应的单词分布中抽取一个单词；</li><li>重复上述过程直至遍历文档中的每一个单词。</li></ol><p>我们通过对所有文档进行LDA训练，就可以构建每篇文档的主题分布，从而构建一个 <em>基于主题的向量（每个主题就是向量的一个分量，而值就是该主题的概率值）</em> ，这样我们就可以利用该向量来计算两篇文档的相似度了。主题模型可以理解为一个降维过程，将文档的词向量表示将维成主题的向量表示（主题的个数是远远小于词的个数的，所以是降维）。</p><p>c. 利用doc2vec算法构建文本相似度<br>doc2vec或者叫做paragraph2vec, sentence embeddings，是一种非监督式算法，可以获得句子、段落、文章的稠密向量表达，它是word2vec的拓展，2014年被Google的两位大牛提出，并广泛应用于文本分类和情感分析中。通过doc2vec学习出句子、段落、文章的向量表示，可以通过计算向量之间距离来表达句子、段落、文章之间的相似性。</p><p>这里我们简单描述一下doc2vec的核心思想。doc2vec受word2vec启发，由它推广而来，word2vec的设计思路是通过学习一个唯一的向量表示每个词，每个词向量作为矩阵W中的一列（W是所有词向量构成的矩阵），矩阵列可以通过词汇表为每个词做索引，排在索引第一位的放到矩阵W的第一列，如此类推。将学习问题转化为通过上下文词序列中前几个词来预测下一个词。具体的模型框架如下图：<br><img src="alg_word2vec.png" alt="word2vec算法框架"></p><p>简单来说，给定一个待训练的词序列，词向量模型通过极大化平均对数概率<br>$$<br>\frac{1}{T}\sum_{t=k}^{T-k} \log p(w_t|w_{t-k}, …, w_{t+k})<br>$$</p><p>将预测任务通过softmax变换看成一个多分类问题</p><p>$$<br>p(w_t|w_{t-k}, …, w_{t+k}) = \frac{e^{y_{w_t}}}{\sum _ie^{y_i}}<br>$$</p><p>上面公式中词i的归一化对数概率，可以用下面公式计算，其中U、b是参数，h是通过词向量的拼接或者平均来构建的。</p><p>$$<br>y = b + Uh(w_{t-k}, …, w_{t+k};W)<br>$$</p><p>word2vec算法随机初始化词向量，通过随机梯度下降法来训练神经网络模型，最终得到每个词的向量表示。</p><p>doc2vec类似，每个段落/文档表示为向量，作为矩阵D的一列，每个词也表示为一个向量，作为矩阵W的一列。将学习问题转化为通过上下文词序列中前几个词和段落/文档来预测下一个词。将段落/文档和词向量通过拼接或者平均来预测句子的下一个词（下图是通过“the”、“cat”、“sat”及段落id来预测下一个词“on“）。在训练的时候我们固定上下文的长度，用滑动窗口的方法产生训练集。段落向量/句向量在上下文中共享。<br><img src="alg_doc2vec.jpeg" alt="doc2vec模型结构"></p><p>工程上有很多开源框架有word2vec和doc2vec的实现，比如gensim中就有很好的实现。<a href="https://radimrehurek.com/gensim/models/doc2vec.html" target="_blank" rel="noopener">参考链接</a>。</p><p><strong>4.图片、音频、或者视频信息</strong><br>如果标的物包含的是图片、音频或者视频信息，处理会更加复杂。一种方法是利用它们的文本信息（标题、评论、描述信息、利用图像技术提取的字幕文本信息、利用语音识别获取的文本信息等）采用（3）的技术方案获得向量化表示。对于图像或者视频，也可以利用opencv中的PSNR和SSIM算法来表示视频特征，也可以计算视频之间的相似度。总之，图片、图像、音频都可以转化为NLP问题或者图像处理问题，通过图像处理和NLP获得对应的特征表示，从而最终计算出相似度，如下图表示。<br><img src="alg_va.jpeg" alt="视频/图片问题转化为NLP或图像处理问题"></p><h2 id="为用户做个性化推荐"><a href="#为用户做个性化推荐" class="headerlink" title="为用户做个性化推荐"></a>为用户做个性化推荐</h2><p>有了上面用户和标的物的特征表示，剩下就是基于此为用户做个性化推荐了，总结了下面5种方法和策略。这里总结的推荐是完全个性化范式的推荐，为每个用户生成不一样的推荐结果。</p><p><strong>1.采用跟基于物品的协同过滤类似的方式推荐</strong><br>该方法采用基于用户行为记录的显示特征表示用户特征，通过将用户操作过的标的物最相似的标的物推荐给用户，算法原理跟基于物品的协同过滤类似，计算公式甚至是一样的，但是这里计算标的物相似度是基于标的物的自身信息来计算的，而基于物品的协同过滤是基于用户对标的物的行为矩阵来计算的。</p><p>用户u对标的物s的喜好度 sim(u, s) 可以采用如下公式计算，其中U是所有用户操作过的标的物列表，是用户u对标的物的喜好度，是标的物与s的相似度。</p><p>$$<br>sim(u, s) = \sum_{s_i \in U} score(s_i) * sim(s_i, s)<br>$$</p><p>有了用户对每个标的物的相似度，基于相似度降序排列，就可以取topN推荐给用户了。<br>除了采用上面的公式外，我们在推荐时也可以稍作变化，采用最近邻方法（K-NearestNeighbor, KNN）。对于用户操作/喜欢过的每个标的物，通过KNN找到最相似的k个标的物</p><p>$$<br>Rec(u) = \sum_{s_i \in U} {s_j|s_j \in kNN(s_i)}<br>$$</p><p>其中Rec(u)是给用户u的推荐，是标的物最近邻（最相似）的k个标的物。</p><p><strong>2.采用跟基于用户协同过滤类似的方法计算推荐</strong><br>如果我们获得了用户的人口统计学向量表示或者基于用户历史操作行为获得了用户的向量化表示，那么我们可以采用跟基于用户的协同过滤方法相似的方法来为用户提供个性化推荐，具体思路如下：<br>我们可以将与该用户最相似的用户喜欢的标的物推荐给该用户，算法原理跟基于用户的协同过滤类似，计算公式甚至是一样的。但是这里计算用户相似度是基于用户的人口统计学特征向量表示来计算的（计算用户向量cosine余弦相似度）或者是基于用户历史行为嵌入获得的特征向量来计算的，而基于用户的协同过滤是基于用户对标的物的行为矩阵来计算用户之间的相似度。</p><p>用户u对标的物s的喜好度 sim(u, s) 采用如下公式计算，其中U是与该用户最相似的用户集合，是用户对标的物s的喜好度，是用户与该用户u的相似度。</p><p>$$<br>sim(u, s) = \sum_{u_i \in U} sim(u, u_i) * score(u_i, s)<br>$$</p><p>有了用户对每个标的物的相似度，基于相似度降序排列，就可以取topN推荐给用户了。</p><p>与前面一样我们也可以采用最近邻方法(K-NearesNeighbor, KNN)。通过KNN找到最相似的k个用户，将这些用户操作/喜欢过的每个标的物推荐给用户。</p><p>$$<br>Rec(u) = \sum_{u_i \in KNN(u)} {s_j \in A(u_i)}<br>$$</p><p>其中Rec(u)是给用户u的推荐， kNN(u)是用户相似的k个用户。是用户操作/喜欢过的标的物的集合。</p><p><strong>3.基于标的物聚类的推荐</strong><br>有了标的物的向量表示，我们可以用kmeans等聚类算法将标的物聚类，有了标的物的聚类，推荐就好办了。从用户历史行为中的标的物所在的类别挑选用户没有操作行为的标的物推荐给用户，这种推荐方式是非常直观的。电视猫的个性化推荐就采用了类似的思路。具体计算公式如下，其中是给用户u的推荐，H是用户的历史操作行为集合，Cluster(s)是标的物s所在的聚类。</p><p>$$<br>Rec(u) = \sum_{s\in H}{t\in Cluster(s) \&amp; t\neq s}<br>$$</p><p><strong>4.基于向量相似的推荐</strong><br>不管是前面提到的用户的显示兴趣特征（利用标签来衡量用户兴趣）或者是向量式的兴趣特征（将用户的兴趣投影到向量空间），我们都可以获得用户兴趣的向量表示。</p><p>如果我们获得了用户的向量表示和标的物向量表示，那么我们就可以通过向量的cosine余弦相似度计算用户与标的物之间的相似度。一样的，有了用户对每个标的物的相似度，基于相似度降序排列，就可以去topN推荐给用户了。</p><p>基于向量的相似的推荐，需要计算用户向量与每个标的物向量的相似性。如果标的物数量较多，整个计算过程相当耗时。同样，计算标的物最相似的K个标的物，也会涉及到与每个其他标的物计算相似度，也非常耗时。真个计算过程的时间复杂度是O(N)，其中N是标的物的总个数。</p><p>上述复杂的计算过程可以利用Spark等分布式计算平台来加速。对于T+1级（每天更新一次推荐结果）的推荐服务，利用Spark提前计算好，将推荐结果存储起来供前端业务调用。</p><p>另外一种可行的策略是利用高效的向量检索库，在极短时间（毫秒级）内为用户所引出topN最相似的标的物。目前facebook开源的<a href="https://github.com/facebookresearch/faiss" target="_blank" rel="noopener">FAISS库</a>就是一个高效的向量搜索与聚类库，可以在毫秒级响应查询及聚类需求，因此可以用于个性化的实时推荐。目前FAISS已经在推荐业务上得到广泛应用。</p><p><a href="https://github.com/facebookresearch/faiss" target="_blank" rel="noopener">FAISS库</a>适合稠密向量的检索和聚类，所以对于利用LDA、doc2vec算法构建向量表示的方案是实用的，因为这些方法构建的是稠密向量。而对于TF-IDF及基于标签构建的向量化，就不适用了，这两类方法构建的都是稀疏高位矩阵。</p><p><strong>5.基于标签的反向倒排索引做推荐</strong><br>基于标的物的标签和用户的历史兴趣，可以构建出用户基于标签兴趣的画像及推荐与标的物的倒排索引查询表。基于该反向索引表及用户的兴趣画像，我们就可以为用户做个性化推荐了。该类算法其实就是基于标签的召回算法。</p><p>具体推荐过程见下图（基于倒排索引的电影推荐）：从用户画像中获取用户兴趣标签，基于用户的兴趣标签从倒排索引中获取该标签对应的标的物，这样就可以从用户关联到标的物了。其中用户的每个兴趣标签及标签关联到的标的物都是有权重的。</p><p><img src="alg_antiindex.jpeg" alt="基于倒排索引的电影推荐"></p><p>假设用户的兴趣标签及对应的标签权重形如 $\{(T_1, S_1), (T_2, S_2), (T_3, S_3), ……, (T_k, S_k)\}$ ，其中T是标签， S是用户对标签的偏好权重。<br>假设标签关联的标的物分别为</p><p>$$<br>T_1 \longleftrightarrow \{(O_{11}, W_{11}), (O_{12}, W_{12}), (O_{13}, W_{13}), ……, (O_{1p_1}, W_{1p_1})\}<br>$$</p><p>$$<br>T_2 \longleftrightarrow \{(O_{21}, W_{21}), (O_{22}, W_{22}), (O_{23}, W_{23}), ……, (O_{2p_2}, W_{2p_2})\}<br>$$</p><p>$$<br>\cdots<br>$$</p><p>$$<br>T_k \longleftrightarrow \{(O_{k1}, W_{k1}), (O_{k2}, W_{k2}), (O_{k3}, W_{k3}), ……, (O_{kp_k}, W_{kp_k})\}<br>$$ </p><p>其中O、K分别是标的物及对应的权重，那么</p><p>$$<br>U=\sum_{i=1}^k S_i*T_i \<br>$$</p><p>$$<br>=\sum_{i=1}^k S_i*\{(O_{i1}, W_{i1}), (O_{i2}, W_{i2}), (O_{i3}, W_{i3}), …, (O_{ip_i}, W_{ip_i})\}<br>$$</p><p>$$<br>=\sum_{i=1}^k\sum_{j=1}^{p_i} S_i * W_{ij} * O_{ij}<br>$$</p><p>上式中U是用户对标的物的偏好集合，这里将标的物看成向量空间的基，所以有上面公式。不同的标签可以关联到相同的标的物（因为不同的标的物可以有相同的标签），上式中最后一个等号右边需要合并同类项，将相同基前面的系数相加。合并同类项后，标的物（基）前面的数值就是用户对该标的物的偏好程度了，我们对这些偏好程度降序排列，就可以为用户做topN推荐了。</p><p><em>以上就是基于内容的推荐算法的核心原理。</em></p><h1 id="基于内容的推荐算法应用场景"><a href="#基于内容的推荐算法应用场景" class="headerlink" title="基于内容的推荐算法应用场景"></a>基于内容的推荐算法应用场景</h1><h2 id="完全个性化推荐"><a href="#完全个性化推荐" class="headerlink" title="完全个性化推荐"></a>完全个性化推荐</h2><p>为每个用户生成不同的推荐结果。</p><h2 id="标的物关联标的物推荐"><a href="#标的物关联标的物推荐" class="headerlink" title="标的物关联标的物推荐"></a>标的物关联标的物推荐</h2><p>即根据当前用户浏览标的物，做相关标的物列表topN推荐。</p><h2 id="配合其他推荐算法"><a href="#配合其他推荐算法" class="headerlink" title="配合其他推荐算法"></a>配合其他推荐算法</h2><p>由于基于内容的推荐算法在精准度上不如协同过滤肃反啊，但是可以更好的适应冷启动，所以在实际业务中基于内容的推荐算法会配合其他算法一起服务于用户，常用方法是采用级联方式，先给用户协同过滤的推荐结果，如果该用户行为较少导致没有协同过滤推荐结果，就为该用户推荐基于内容的推荐算法产生推荐结果。</p><h2 id="主题推荐"><a href="#主题推荐" class="headerlink" title="主题推荐"></a>主题推荐</h2><p>如果我们有标的物的推荐信息，并且基于推荐系统构建了一套推荐算法，那么我们就可以将用户喜欢的标签采用主题的方式推荐给用户，每个主题就是用户的一个兴趣标签。通过一系列主题罗列展示，让用户从中筛选自己感兴趣的内容。下图呈现了Netflix首页的示例。<br><img src="alg_netflix.jpeg" alt="Netflix首页推荐"></p><p>Netflix的首页大量采用基于主题的推荐模式。主题推荐的好处是可以将用户所有的兴趣点按照兴趣偏好大小先后展示出来，可解释性强，并且让用户有更多维度的自由选择空间。</p><p>在真实产品中可以采用比netflix这个示例首页更好的方式。具体来说，可以为每个标签通过人工编辑生成一句更有表达空间的话（如武侠标签，可以采用“江湖风云再起，各大门派齐聚论剑”这样更有深度的表述），具体前端展示映射到人工填充的话而不是直接展示原来的标签。</p><h2 id="给用户推荐标签"><a href="#给用户推荐标签" class="headerlink" title="给用户推荐标签"></a>给用户推荐标签</h2><p>另外一种可行的推荐策略是不直接给用户推荐标的物，而是给用户推荐标签，用户通过关注推荐的标签，自动获取具备该标签的标的物。除了可以通过推荐的标签关联到标的物获得直接推荐标的物类似的效果外，间接的通过用户对推荐的标签选择、关注等行为进一步获得了用户的兴趣偏好，这是一种更简简单可行的推荐产品实现方案。</p><h1 id="基于内容的推荐算法的优缺点"><a href="#基于内容的推荐算法的优缺点" class="headerlink" title="基于内容的推荐算法的优缺点"></a>基于内容的推荐算法的优缺点</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ol><li><p>可以很好的识别用户偏好<br>基于内容的推荐算法完全基于用户的历史兴趣来做推荐，推荐的标的物也跟用户的历史兴趣相似，所以推荐内容更加符合用户偏好。</p></li><li><p>直观易懂，解释性强<br>算法基于用户的兴趣为其推荐相似标的物，原理简单、容易理解。同时，由于是基于用户历史兴趣推荐跟兴趣相似的标的物，用户也非常容易接受和认可。</p></li><li><p>更加容易的解决冷启动<br>只要用户有一个系统操作系统，就可以基于内容为用户做推荐，随着行为加深，逐渐优化。同时对于新入库的标的物，只要它具备metadata信息等标的物相关信息，就可以利用基于内容的推荐算法将它分发出去。因此，对于强依赖于UGC内容的产品（抖音等），基于内容的推荐可以更好的对标的物提供方进行流量扶持。</p></li><li><p>算法实现相对简单<br>基于内容的推荐可以基于标签维度做推荐，也可以将标的物嵌入向量空间中，利用相似度做推荐，不管哪种方式，算法实现简单，有现成的开源算法库，容易落地。</p></li><li><p>对于小众领域也能有较好的推荐效果<br>对于冷门小众标的物，用户行为少，协同过滤等方法很难将这类内容分发出去，而基于内容的推荐算法受影响较小。</p></li><li><p>适合标的物快速增长的有时效性要求的产品<br>对于标的物增长很快的产品，如今日头条等新闻类，每天都有海量标的物入库，实效性也很强。新的标的物一般用户行为少，协同过滤算法很难将这些大量实时产生的新的标的物推荐出去，这时就可以采用基于内容的推荐算法更好的分发这些内容</p></li></ol><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ol><li><p>推荐范围狭窄，新颖性不强<br>由于该类算法只依赖于单个用户的行为为用户做推荐，推荐的结果会聚集在用户过去感兴趣的标的物类别上，如果用户不主动关注其他类型的标的物，很难为用户推荐多样性的结果，也无法挖掘用户深层次的潜在兴趣。特别对于新用户，只有少量行为，为用户推荐的标的物会比较单一。</p></li><li><p>需要知道相关的内容信息且处理起来较难<br>内容信息主要是文本、视频、音频，处理起来费力，相对难度较大，依赖领域知识。同时这些信息大概率含有噪音，增加处理难度。另外，对内容理解的全面性、完整性及准确性会影响推荐的效果。</p></li><li><p>较难将长尾标的物分发出去<br>基于内容的推荐需要用户对标的物有操作行为，长尾标的物一般操作行为少。由于基于内容的推荐只利用单个用户行为做推荐，所以更难将它分发给更多的用户。</p></li><li><p>推荐精准度不太高<br>相比协同过滤算法，基于内容的推荐算法精准度要差一些。</p></li></ol><h1 id="基于内容的推荐算法落地需要关注的重要问题"><a href="#基于内容的推荐算法落地需要关注的重要问题" class="headerlink" title="基于内容的推荐算法落地需要关注的重要问题"></a>基于内容的推荐算法落地需要关注的重要问题</h1><p>基于内容的推荐算法虽然容易理解、实现相对简单，但落地过程中，总结出来如下关键问题需要提前思考。</p><h2 id="内容来源的获取"><a href="#内容来源的获取" class="headerlink" title="内容来源的获取"></a>内容来源的获取</h2><ol><li><p>标的物“自身携带”的信息<br>标的物在上架时，第三方会准备相关的内容信息，如天猫上的商品在上架时会补充很多必要的信息。对于音视频，各类metadata信息也是入库上架时需要填充的信息。我们要做的是增加对新标的物入库的监控和审核，及时发现信息不全的情况并做适当处理。</p></li><li><p>通过爬虫获取标的物相关信息<br>通过爬虫爬取的信息可以作为标的物信息的补充，增加标的物特征完整表示能力的数据来源。</p></li><li><p>通过人工标注数据<br>往往人工标注的数据价值密度高，通过人工精准的标注可以大大提升算法推荐的精准度。问题是成本高。</p></li><li><p>通过运营活动或者产品交互让用户填的内容<br>通过抽奖活动让用户填写家庭组成、兴趣偏好等，在用户开始注册时让用户填写兴趣偏好特征，这些都是获取内容的手段。</p></li><li><p>通过收集用户行为直接获得或者预测推断出的内容<br>通过请求用户GPS位置知道用户的活动轨迹，用户购买时填写收货地址，用户绑定的身份证和银行卡等，通过用户操作行为预测出用户的兴趣偏好，这些方法都可以获得部分用户数据。</p></li><li><p>通过与第三方合作或者产品矩阵之间补充信息<br>目前中国有大数据交易市场，通过正规的数据交易或者跟其他公司合作，在不侵犯用户隐私的情况下，通过交换数据可以有效填补自己产品上缺失的数据。<br>如果公司有多个产品，新产品可以借助老产品的巨大用户基数，将新产品的用户与老产品用户关联起来(id-maping或者账号打通)，这样老产品上丰富的用户行为信息可以赋能给新产品。</p></li></ol><h2 id="利用负面反馈"><a href="#利用负面反馈" class="headerlink" title="利用负面反馈"></a>利用负面反馈</h2><p>用户对标的物的操作行为不一定代表正向反馈，有可能是负向的。比如点开一个视频，看了不到几秒就退出来了，明显表明用户不喜欢。有很多产品会在用户交互中直接提供负向反馈能力，这样可以收集到更多负向反馈。下面是今日头条和百度APP推荐的文章，右下角有一个小叉叉(见下面图9中红色圈圈)，点击后展示上面的白色交互区域，读者可以勾选几类不同的负向反馈机制。<br><img src="alg_negative.jpeg" alt="负面反馈形式"></p><p>负面反馈一般代表用户强烈的不满，因此利用好负反馈信息可以大大提升推荐系统的精准度和满意度。基于内容的推荐算法整合负反馈方式一般有以下几种：</p><ol><li><p>将负向反馈整合到算法模型中<br>在构建算法模型中整合负向反馈，跟正向反馈一起学习，从而更自然的整合负向反馈信息。</p></li><li><p>采用事后过滤方式<br>先生成推荐列表，再从该列表中过滤掉负向反馈关联或者相似的标的物。</p></li></ol><h2 id="兴趣随时间变化"><a href="#兴趣随时间变化" class="headerlink" title="兴趣随时间变化"></a>兴趣随时间变化</h2><p>用户的兴趣不是一成不变的，算法如何整合用户的兴趣变化？可行的策略是对用户的兴趣根据时间衰减，将用户操作行为投射到时间线上，并根据时间线的先后顺序给予不同权重，同时给用户建立短期兴趣特征和长期（一般）兴趣特征，在推荐时既考虑短期有考虑长期兴趣，最终推荐列表中整合两部分结果呈现。</p><h2 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h2><p>基于内容的推荐算法依赖于标的物相关的描述信息，这些信息更多的是以文本的形式存在，这就涉及到自然语言处理，文本中可能会存在很多歧义、符号、脏数据，我们需要实现对数据进行很好的处理，才能让后续的推荐算法产生好的效果。</p><h2 id="加速计算与节省资源"><a href="#加速计算与节省资源" class="headerlink" title="加速计算与节省资源"></a>加速计算与节省资源</h2><p>在实际推荐算法落地时，我们会事先为每个标的物计算N(=50)个最相似的标的物，事先将计算好的标的物存起来，减少时间和空间成本，方便后续更好地做推荐。同时也可以利用各种分布式计算平台和快速查询平台(如Spark、FAISS库等)加速计算过程。另外，算法开发过程中尽量做到模块化，对业务做抽象封装，这可以大大提升开发效率，并且可能会节省很多资源。</p><h2 id="解决基于内容的推荐越推越窄的问题"><a href="#解决基于内容的推荐越推越窄的问题" class="headerlink" title="解决基于内容的推荐越推越窄的问题"></a>解决基于内容的推荐越推越窄的问题</h2><p>前面提到基于内容的推荐存在越推越窄的缺点，那怎么避免或者减弱这种影响呢？当然用协同过滤等其他算法是一个有效的方法。另外，我们可以给用户做兴趣探索，为用户推荐兴趣之外的特征关联的标的物，通过用户的反馈来拓展用户兴趣空间，这类方法就是强化学习中的EE方法。如果我们构造了标的物的知识图谱系统，我们就可以通过图谱拓展标的物更远的联系，通过长线的相关性来做推荐，同样可以有效解决越推越窄的问题。</p><h2 id="工程落地技术选型"><a href="#工程落地技术选型" class="headerlink" title="工程落地技术选型"></a>工程落地技术选型</h2><p>本篇文章主要讲的是基于内容的推荐系统的算法实现原理，具体工程实践时，需要考虑到数据处理、模型训练、分布式计算等技术，当前很多开源方案可以使用，常用的如Spark mllib，scikit-learn，Tensorflow，pytorch，gensim等，这些工具都封装了很多数据处理、特征提取、机器学习算法，我们可以基于第二节的算法思路来落地实现。</p><h2 id="业务的安全性"><a href="#业务的安全性" class="headerlink" title="业务的安全性"></a>业务的安全性</h2><p>除了技术外，在推荐产品落地中还需要考虑推荐的标的物的安全性，避免推荐反动、色情、标题党、低俗内容，这些就需要基于NLP或者CV技术对文本或者视频进行分析过滤。如果是UGC平台型的产品，还需要考虑怎么激励优质内容创作者，让好的内容得到更多的分发机会，同时对产生劣质内容的创作者采取一定的惩罚措施，比如限制发文频率、禁止一段时间的发文权限等。</p><p>转载自 <a href="https://blog.csdn.net/qq_43045873/article/details/93816108" target="_blank" rel="noopener">基于内容的推荐算法</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
            <tag> 内容推荐 </tag>
            
            <tag> 向量化表示 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>理解“无穷小”</title>
      <link href="/2021/10/25/about-infinitesimal/"/>
      <url>/2021/10/25/about-infinitesimal/</url>
      
        <content type="html"><![CDATA[<blockquote><p>九城爱数学</p></blockquote><h1 id="什么是无穷小？"><a href="#什么是无穷小？" class="headerlink" title="什么是无穷小？"></a>什么是无穷小？</h1><p>无穷小数学符号记为 $\epsilon$ , 它的数学性质应该满足：<br><strong>对于任意给定实数 $\gamma$&gt;0</strong>，都满足 0&lt;|$\epsilon$|&lt;$\gamma$。很明显，无穷小是数学概念，并不可能存在于实数域中。<br>以极限形式定义会得到更加简洁直观的表达：满足$\lim\limits_{x\to c}f(x) = 0$ 的 $f(x)$ 是 $x\to c$ 时的无穷小。</p><p>无穷小有一个重要的隐含性质，即参考系无关，类似物理定律，在任意参照物系统里面，都绝对成立。<br>那么无穷小到底是一个数么？以反证法来说明，如果无穷小是一个数，那么这个数一定不会小于它本身，则违反了上述无穷小定义公式，所以无穷小并不是一个数。<br>实际上，在经典的极限和微积分定义中，无穷小通常是以函数、数列形式表达，可以自由表达。</p><h1 id="关于0-and-无穷小"><a href="#关于0-and-无穷小" class="headerlink" title="关于0 and 无穷小"></a>关于0 and 无穷小</h1><p>0，你可以看成是常数，也可以是常数函数 $f(x)=0$ 或者常数数列 $ ${$0, 0, 0, …$}$ $，都符合无穷小的定义，绝对值小于任意正实数。<br>无穷小与0一个最重要的区别是数学上，无穷小可以用作除法，两个相关（相关的意义是具有函数关系）的无穷小的比值就是这个函数的导数。</p><h1 id="无穷小-and-物理"><a href="#无穷小-and-物理" class="headerlink" title="无穷小 and 物理"></a>无穷小 and 物理</h1><p>数学意义上的无穷小，是一个混乱且容易引起歧义的量，正在逐渐消失在视野；但是在屋里中，这个无穷小确是切实存在的。<br>物理学的意义是用数学描述现实世界的科学，一个重要区别是数学本质是连续的，数学上的离散化是在连续基础上界定和设计出来，而物理学对现实世界的描述过程中，引发了多次革命，最为重要的一次革命就是现实世界的离散化（量子化），而这个离散化后的常量值就是物理世界中的无穷小，即普朗克常数。</p><p>** 刚开始逼着写博客，有点累，休息了。实际上未完。。。 **</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 极限 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于EIP-1559协议</title>
      <link href="/2021/10/24/eip1559/"/>
      <url>/2021/10/24/eip1559/</url>
      
        <content type="html"><![CDATA[<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>比特币以一种全新的思路重建了货币体系，这个传统情况下被认为一定需要权威中心（政府）背书和管理的核心金融基础设施，以太坊则带来了由加密货币到去中心化应用生态爆发，从ICO到Defi，再到NFT，区块链已经越来越接近真实生活。秉承去中心化理念的全面应用，以太坊升级也是去中心化的，任何一个feature改进或升级都需要参与者共同确认，因为EIP（以太坊改进提案）应运而生，而EIP1559则可视为以太坊最重要的提案之一，因为它的目的是改革ETH的Gas费定价机制，让用户以合理可接受的成本使用以太坊。</p><h1 id="以太坊提案EIP-1559"><a href="#以太坊提案EIP-1559" class="headerlink" title="以太坊提案EIP-1559"></a>以太坊提案EIP-1559</h1><p>EIP-1559是以太坊的重要提案，它对未来ETH的价值捕获、用户体验、安全性等方面都会产生重大影响。中短期内，EIP-1559提案对ETH的价格冲击力可能仅次于ETH的PoS，从长期看，可能会有更持久的影响力。</p><h2 id="什么是EIP-1559？"><a href="#什么是EIP-1559？" class="headerlink" title="什么是EIP-1559？"></a>什么是EIP-1559？</h2><p>EIP-1559本质上是关于以太坊网络交易定价机制的解决方案，它包括每区块网络费用的固定部分base fee（也就是基础费用，会被销毁，矿工收不到这笔费用），同时还有动态的可伸缩的区块大小设计，以应对瞬时的网络拥堵。</p><p>虽然每区块的基础费用是固定的，但是它会根据网络的拥堵情况，调节每区块的基础费用，它有一个公式用来调整基础费用的上升或下降。它会根据上一个区块所用的gas和目标gas（gas target，也就是之前的gas limit）来调整。当区块高于目标gas价格，基础费用上升，当区块低于目标gas价格，基础费用下降。</p><p>除了基础费用，还可以有打赏费用（小费）。在拥堵时，小费用于激励矿工将用户交易打包进区块。交易可以指定基础费用和小费的上限。这一提案还包括过渡性方案。开始时，区块的一半保留原来的竞价机制，一半采用新的费用机制，并逐渐过渡到新的方案。</p><p>这里有几个关键点：</p><p>费用结构</p><p>基本费用+小费</p><p>费用流向</p><p>基本费用会被销毁，矿工无法获得；矿工的收益主要是新增区块奖励+小费</p><p>弹性区块</p><p>可伸缩区块用以应对网络拥堵，同时基本费用也根据拥堵情况进行调整。</p><p><img src="eip1559.jpeg" alt="eip1559提案"></p><h2 id="为什么要搞EIP-1559？"><a href="#为什么要搞EIP-1559？" class="headerlink" title="为什么要搞EIP-1559？"></a>为什么要搞EIP-1559？</h2><p>对发生在以太坊上的交易，目前的收费方案是拍卖机制。由用户出价，矿工选择出价最高的交易，将其打包进区块。这种方式看上去简单且高效，不过也有问题：</p><p>竞价的低效率</p><p>以太坊上的最低出价可能会存在极大的差异。这导致出价效率低下。用户发送交易时选择出价的费用上限，而矿工选择最高出价的交易。对用户来说，出价多少合适是很难预估的，即便有复杂的费用预估算法，也不太可能做到很好的估算，经常会出现超额费用支付的情况。</p><p>造成延误</p><p>由于每区块gas limit的限制和交易量的自然波动，这会导致交易可能需要等待几个区块才能被打包进入。对用户来说，可能会造成延误。而EIP-1559提出弹性区块的机制，可以让一个区块变大，而下一个区块变小，实现更长期的平均区块大小限制，允许不同区块存在一定的大小差异。这解决了当前一些区块过满，而一些区块使用过少的现状。</p><p>安全性</p><p>随着区块奖励的减少（如比特币和Zcash），未来交易费成为奖励矿工的主要来源。因此需要有足够的交易规模来支撑网络的安全，同时还可能会导致矿工的自私挖矿等问题，产生一些不稳定因素。</p><p>EIP-1559为了解决上述问题，它改变了付费结构和付费流向：将交易费用分为基本费用+小费，同时基本费用被销毁，矿工无法获得，矿工可以获得打赏费用，也可以获得新增发的区块奖励，从而减少矿工操纵交易费用的动机，并让包括矿工在内各种参与者有机会获益。</p><p>此外，在EIP-1559的设计中，当网络超出每区块目标gas使用量，基本费用会在接下来的区块增加，而当容量低于目标gas使用量，则下降。因此，gas费用的变化是根据需求情况进行调整的，其区块之间的费用差异是可预测的。</p><p>这对于用户体验来说会有提升。EIP-1559实施之前的以太坊采用的是类似于比特币的第一价格拍卖机制（First Price Auctions），其缺点之一是需要进行费用估算。而EIP-1559则对所有交易尽可能实施相同的费率。用户需要决定的是是否支付费用，而不用太多考虑出价多少。钱包可以帮助用户预测基本费用以及自动设置一点打赏费用，减少了大多数用户手动调整gas费用的操作。当然，如果用户手动设置交易费用上限也是可以的,用户通过手动设置费用上限可以限制其最高成本。</p><p>除了用户体验提升，还可以提升以太坊的安全性以及解决经济抽象等问题。</p><p>由于矿工在交易费用中仅仅捕获打赏费用，而基本费用被协议销毁，这样可以消除矿工操作费用以获得更多费用的动机。更重要的是，它改变了ETH的价值捕获机制。之前的ETH更多代表了以太坊上价值流通和价值存储，本身并没有捕获费用价值。</p><p>而基本费用改变了这一局面。基本费用的捕获意味着ETH开始捕获以太坊网络的交易费用，而交易费用的多少取决于以太坊上交易规模。随着以太坊交易规模的增大，ETH可以捕获的费用就越多。</p><p>同时，在EIP-1559实施前的以太坊交易费用支付中，并不是一定得用ETH支付。而实施之后基本费用是使用ETH支付，这也在一定程度上解决了之前讨论的“经济抽象”的问题。所谓的“经济抽象”，就是说用户可以使用稳定币或其他代币支付手续费。这不仅会威胁原生代币ETH在网络中的位置，而且也不利于网络安全。EIP-1559要求消耗特定数量的ETH，会增加经济抽象的难度。</p><p>如果以太坊上的交易规模足够大，那么它可以捕获大量的基本费用，这些用ETH支付的基本费用会被销毁，这会降低ETH的通胀率，甚至可能通缩的情况（假设增发低于销毁）。这有利于ETH的价值支撑。</p><p>从安全性来看，在比特币的激励中，当区块补贴完毕，它主要依靠交易费用来维持其安全，这种模式如何获得可持续目前还是存在争议的。而以太坊通过将交易手续费销毁，并将矿工激励通过区块奖励持续运作下去。这是一个更为弹性的机制。由于交易手续费销毁，价值转移到ETH上，以太坊采用ETH区块奖励对矿工进行补贴，这为以太坊安全的长期可持续提供了基础。</p><h2 id="EIP-1559对ETH价值的支撑"><a href="#EIP-1559对ETH价值的支撑" class="headerlink" title="EIP-1559对ETH价值的支撑"></a>EIP-1559对ETH价值的支撑</h2><p>通过过去一段时间的实践，我们可以看到单纯挖矿代币的价值支撑存在很大的问题。有些挖矿奖励代币，或者说是所谓的“治理代币”，有很大的抛压，但价值支撑不够，最终导致币价下跌，激励下降，甚至出现了不少死亡螺旋的代币。</p><p>有些头部DEX交易量很好，流动性也不错，协议的年化手续费用也非常高，基本面一片大好，但同样也无法支撑起价值。因为它们也同样面临“挖卖提”的情况。其代币本身只是“治理代币”，没有捕获协议价值。目前的协议费用中，绝大部分收益依然没有融入到代币中来，而是直接分给LP。这意味着代币和协议的价值成长并没有达成高度融合。这就是在当前情况下它们的价值无法支撑的原因所在。当然，未来这种局面可能会随着治理的深入发生改变，因为治理本身可以改变这个局面。目前的治理代币更多是代表了未来的可能性。</p><p>EIP-1559对ETH的影响也是类似的。EIP-1559实施之前的ETH并没有捕获到以太坊网络费用的价值，这导致ETH在泡沫破灭时完全无法实现价值的支撑，从泡沫时最高的近1500美元跌至底点的100多美元。</p><p>而EIP-1559之前的ETH将捕获以太坊网络的费用价值，截止到蓝狐笔记写稿时，以太坊的年化网络费用超过了13亿美元，这是一笔不小的费用。</p><p>这意味着，未来ETH除了作为价值存储和价值媒介之外，还可以真正捕获到大规模的价值。就其当前价值捕获的规模看，它是目前所有公链中最强势的，这让ETH在未来有更坚实的价值支撑。</p>]]></content>
      
      
      <categories>
          
          <category> 区块链 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 以太坊 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>enjoy new world</title>
      <link href="/2021/10/15/enjoy-new-world/"/>
      <url>/2021/10/15/enjoy-new-world/</url>
      
        <content type="html"><![CDATA[<blockquote><p>关于写博客</p></blockquote><p><img src="/medias/contact.jpg" alt></p><h2 id="为什么要创建自己的博客站"><a href="#为什么要创建自己的博客站" class="headerlink" title="为什么要创建自己的博客站"></a>为什么要创建自己的博客站</h2><p><strong><em>曾经对博客不屑，但到了该记录的年纪</em></strong></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
