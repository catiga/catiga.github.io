<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>关于知识图谱构建</title>
      <link href="/2021/12/03/knowledge-graph/"/>
      <url>/2021/12/03/knowledge-graph/</url>
      
        <content type="html"><![CDATA[<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>从搜索引擎开始，到内容推荐、智能交互等众多领域，传统算法越来越难以满足用户需求的快速迭代。与人工智能相关技术的结合，上述领域的底层技术都与领域内知识图谱构建相关。<br>知识图谱的概念是谷歌在2012年提出，应用于智能搜索领域，并且逐渐在学术界和工业界普及。随着信息服务应用的智能化需求越来越高，知识图谱已经广泛应用于智能搜索、智能问答、个性化推荐、舆情分析、艺术创作等领域。通过将互联网上的信息、数据以及关系聚集为知识，借助知识图谱存储引擎，更易于计算、理解与评价，并且形成一套语义知识库。知识图谱以其强大的语义处理能力与开放互联能力，可为知识互联奠定知识的基础。</p><h1 id="知识图谱定义"><a href="#知识图谱定义" class="headerlink" title="知识图谱定义"></a>知识图谱定义</h1><p>知识图谱，是结构化的语义知识库，用于迅速描述物理世界中的概念及相互关系。<br>知识图谱通过对错综复杂的文档数据进行有效加工、处理、整合，转化为简单、清晰的“实体，关系，实体”的三元组，最后聚合大量知识，从而实现知识的快速响应和推理。</p><p>知识图谱有自顶向下和自底向上两种构建方式。自顶向下构建是借助百科类网站等结构化数据源，从高质量数据中提取本体和模式信息，加入到知识库中；自底向上构建是借助技术手段，从公开采集的数据中提取出资源模式，选择其中置信度较高的新模式，经人工审核之后，加入到知识库中。</p><p><img src="kg_pic.jpeg" alt="知识图谱案例"></p><p>如上图所示，节点之间如果存在关系，会被一条无向边连接在一起，这个节点，我们称为 <strong>实体（Entity）</strong>，他们之间的边，称为 <strong>关系（Relationship）</strong>。</p><p>知识图谱的基本单位，便是“实体（Entity）-关系（Relationship）-实体（Entity）”构成的三元组，这也是知识图谱的核心。</p><ul><li>实体：是具有可区别性且独立存在的某种事物。实体是知识图谱中的最基本元素，不同的实体间存在不同的关系。如图中的“中国”、“北京”、“16410平方公里”等。</li><li>关系：关系是连接不同的实体，指代实体之间的联系。通过关系节点把知识图谱中的节点连接起来，形成一张大图。如途中的“人口”、“首都”、“面积”等。</li></ul><h1 id="数据类型和存储方式"><a href="#数据类型和存储方式" class="headerlink" title="数据类型和存储方式"></a>数据类型和存储方式</h1><p>知识图谱的原始数据类型一般来说有三类：</p><ul><li><strong>结构化数据（Structed Data）</strong>：比如关系数据库数据</li><li><strong>半结构化数据（Semi-Structed Data）</strong>：XML、JSON、百科等</li><li><strong>非结构化数据（UnStructed Data）</strong>：图片、音频、视频、文本</li></ul><p>对于这三种数据类型的存储，可以有两种方法选择：</p><ul><li>通过RDF（资源描述框架）这样的规范存储格式来进行存储</li></ul><pre class="line-numbers language-xml"><code class="language-xml"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>RDF</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>Description</span> <span class="token attr-name">about</span><span class="token attr-value"><span class="token punctuation">=</span><span class="token punctuation">"</span>http://www.w3school.com.cn/RDF<span class="token punctuation">"</span></span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>author</span><span class="token punctuation">></span></span>David<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>author</span><span class="token punctuation">></span></span>        <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>homepage</span><span class="token punctuation">></span></span>http://www.w3school.com.cn<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>homepage</span><span class="token punctuation">></span></span>    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>Description</span><span class="token punctuation">></span></span><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>RDF</span><span class="token punctuation">></span></span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>通过图数据库进行存储，如Neo4j等。<br><img src="neo4j.png" alt="图数据库"></li></ul><p>在知识图谱方面，图数据库比关系数据库灵活的多。在数据少的时候差别不大，但知识图谱复杂并且涉及到2、3层关联度查询时，图数据库的查询效率会比关系数据库的效率高出几千甚至几百万倍。</p><h1 id="知识图谱的架构"><a href="#知识图谱的架构" class="headerlink" title="知识图谱的架构"></a>知识图谱的架构</h1><p>架构可拆分为逻辑架构和技术架构。</p><h2 id="逻辑架构"><a href="#逻辑架构" class="headerlink" title="逻辑架构"></a>逻辑架构</h2><p>知识图谱在逻辑上可分为模式层与数据层两个层次。</p><ul><li>模式层构建在数据层之上，是知识图谱的核心，通常采用本体库来管理知识图谱的模式层。本体是结构化知识库的概念模板，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度较小。</li></ul><p><strong>模式层：实体-关系-实体，实体-属性-性值</strong></p><ul><li>数据层主要是由一系列的事实组成，而知识将以事实为单位进行存储。如果用（实体1，关系，实体2）、（实体、属性、属性值）这样的三元组来表达事实，可选择图数据库作为存储介质，例如开源的Neo4j、Twitter的FlockDB、sones的GraphDB等。</li></ul><p><strong>数据层：比尔盖茨-妻子-梅琳达·盖茨，比尔盖茨-总裁-微软</strong></p><h2 id="技术架构"><a href="#技术架构" class="headerlink" title="技术架构"></a>技术架构</h2><p>知识图谱的整体架构如下图所示，其中虚线框内部分为知识图谱构建过程，同时也是知识图谱更新的过程。<br><img src="kg_tech.png" alt="知识图谱技术架构"></p><ol><li>虚拟框的最左边是三种输入数据结构，结构化数据、半结构化数据、非结构化数据。这些数据可以来自任何地方，只要它对要构建的这个知识图谱有帮助。</li><li>虚拟框里面的是整个知识图谱的构建过程。其中主要包含了3个阶段，信息抽取、知识融合、知识加工。</li><li>最右边是生成的知识图谱，而且这个技术架构是循环往复，迭代更新的过程。知识图谱不是一次性生成，是慢慢积累的过程。</li></ol><ul><li>信息抽取：从各种类型的数据源中提取出实体、属性以及实体间的相互关系，在此基础上形成本体化的知识表达；</li><li>知识融合：在获得新知识之后，需要对其进行整合，以消除矛盾和歧义，比如某些实体可能有多种表达，某个特定称谓也许对应于多个不同的实体等；</li><li>知识加工：对于经过融合的新知识，需要经过质量评估之后（部分需要人工参与甄别），才能将合格的部分加入到知识库中，以确保知识库的质量。</li></ul><p><img src="kg_tech_stage.png" alt="知识图谱构建阶段划分"></p><p>从上图看出，知识图谱的构建，本质就是信息抽取（抓取、分词、词性）、知识融合（理解语义）、知识加工（表征）三个过程或模块。</p><h1 id="信息抽取"><a href="#信息抽取" class="headerlink" title="信息抽取"></a>信息抽取</h1><p>信息抽取（information extraction）是知识图谱构建的第一步，其中的关键问题是如何从异构数据源中自动抽取信息。信息抽取是自动化的从结构化和非结构化数据（音频/视频/图片）中抽取实体（entity）、关系（relationship）以及属性（attribute）等结构化信息的技术。</p><p>信息抽取涉及到的关键技术包括：实体抽取、关系抽取和属性抽取。</p><h2 id="实体抽取（entity-extraction）"><a href="#实体抽取（entity-extraction）" class="headerlink" title="实体抽取（entity extraction）"></a>实体抽取（entity extraction）</h2><p>实体抽取即命名实体识别（named entity recognition，NER），是指从文本数据集中自动识别出命名实体。实体抽取的质量（准确率和召回率）对知识获取效率和质量影响极大，因此是信息抽取中基础和关键部分。</p><p><img src="kg_ner.jpeg" alt="实体抽取"></p><p>2012年Ling等人归纳出12种实体类别，并基于条件随机场CRF进行实体边界识别，最后采用自适应感知机算法实现了对实体的自动分类，取得了不错的效果。<br>但是随着互联网内容的动态变化，采用人工预定义实体分类体系的方式难以适应需求，因此提出面向开放域的实体识别和分类研究。<br>在面向开放域的实体识别和分类研究中，不需要（也不可能）为每个领域或者每个实体类别建立单独的语料库作为训练集。因此，该领域面临的主要挑战是如何从给定的少量实体实例中自动发现具有区分力的模型。<br>一种思路是根据已知的实体实例进行特征建模，利用该模型处理海量数据集得到新的命名实体列表，然后针对新实体建模，迭代地生成实体标注语料库。<br>另一种思路是利用搜索引擎的服务器日志，事先并不给出实体分类等信息，而是基于实体的语义特征从搜索日志中识别出命名实体，然后采用聚类算法对识别出的实体进行聚类。</p><h2 id="关系抽取（relation-extraction）"><a href="#关系抽取（relation-extraction）" class="headerlink" title="关系抽取（relation extraction）"></a>关系抽取（relation extraction）</h2><p>文本语料经过实体抽取，得到的是一系列离散的命名实体，为了得到语义信息，还需要从相关的语料中提取实体之间的关联关系，通过关联关系将实体（概念）联系起来，才能够形成网状的知识结构，研究关系抽取技术的目的，就是解决如何从文本语料中抽取实体间的关系这一基本问题。</p><p><img src="kg_re.jpeg" alt="关系抽取"></p><p>关系抽取涉及到的技术环节包括：</p><ol><li>人工构造语法和语义规则（模式匹配）</li><li>统计机器学习方法</li><li>基于特征向量或核函数的有监督学习方法</li><li>研究重点转向半监督和无监督</li><li>开始研究面相开放域的信息抽取方法</li><li>将面向开放域的信息抽取方法和面向封闭领域的传统方法结合</li></ol><h2 id="属性抽取（attribute-extraction）"><a href="#属性抽取（attribute-extraction）" class="headerlink" title="属性抽取（attribute extraction）"></a>属性抽取（attribute extraction）</h2><p>属性抽取的目标是从不同信息源中采集特定实体的属性信息。例如针对某个公众人物，可以从网络公开信息中得到其昵称、生日、国籍、教育背景等信息。属性抽取技术能够从多种数据来源中汇集这些信息，实现对实体属性的完整提取。</p><ol><li>将实体的属性视作实体与属性值之间的一种名词性关系，将属性抽取任务转化为关系抽取任务。</li><li>基于规则和启发式算法，抽取结构化数据</li><li>基于百科类网站的半结构化数据，通过自动抽取生成训练语料，用于训练实体属性标注模型，然后将其应用于非结构化数据的实体属性抽取。</li><li>采用数据挖掘的方法直接从文本中挖掘实体属性和属性值之间的关系模式，据此实现对属性名和属性值在文本中的定位。</li></ol><h1 id="知识融合"><a href="#知识融合" class="headerlink" title="知识融合"></a>知识融合</h1><p><img src="kg_tech_stage.png" alt><br>通过信息抽取，从原始非结构化和半结构化数据中获取到了实体、关系、以及实体的属性信息。<br>这些实体和关系以及属性是信息碎片，知识融合的本质就是用正确的语义理解将碎片拼接（连接）起来，形成完整拼图。<br>经过信息抽取，大量碎片是平行化存储的，缺乏层次和逻辑；同时还存在大量冗余和错误的碎片（信息）。<br>知识融合就是为了解决这些问题，涉及到两部分技术框架：实体连接、知识合并。</p><h2 id="实体连接"><a href="#实体连接" class="headerlink" title="实体连接"></a>实体连接</h2><p><strong>实体连接（entity linking）</strong>：是指对于从文本中抽取得到的实体对象，将其链接到知识库中对应正确实体对象的操作。<br>基本思想是首先根据给定的实体项，从知识库中选出一组候选实体对象，然后通过相似度计算将给定的实体项链接到正确的实体对象。</p><p>研究历史：</p><ol><li>仅关注如何将文本中抽取到的实体链接到知识库中，忽视了位于同一文档的实体存在的语义联系。</li><li>开始关注利用实体的共现关系，同时将多个实体链接到知识库中。即集成实体链接（collective entity linking）。</li></ol><p>实体链接的流程：</p><ol><li>从文本中通过实体抽取得到实体项</li><li>进行<strong>实体消歧</strong>和<strong>共指消解</strong>，判断知识库中的同名实体与之是否代表不同的含义以及知识库中是否存在其他命名实体与之表示相同的含义。</li><li>在确认知识库中对应的正确实体对象之后，将该实体项链接到知识库中对应实体。</li></ol><ul><li><p>实体消歧：解决同名实体产生歧义问题的技术，通过实体消歧，可以根据当前的语境，准确建立实体链接，实体消歧主要采用聚类算法。也可以看作基于上下文的分类问题，类似于词性消歧和词义消歧。</p></li><li><p>共指消解：解决多个指称项目对应同一实体对象的问题。在一次会话中，多个指称项可能指向的是同一实体对象。利用共指消解技术，可以将这些指称项关联（合并）到正确的实体对象，该问题在信息检索和自然语言处理等领域具有特殊的重要性。共指消解还有其他名字，如对象对齐、实体匹配和实体同义等。</p></li></ul><h2 id="知识合并"><a href="#知识合并" class="headerlink" title="知识合并"></a>知识合并</h2><p>在构建知识图谱时，可以从第三方知识库产品或已有结构化数据获取知识输入。常见的知识合并需求有两个，一个是合并外部知识库，另一个是合并关系数据库。</p><p>将外部知识库融合到本地知识库需要处理两个层面的问题：</p><ul><li>数据层的融合，包括实体的指称、属性、关系以及所属类别等，主要的问题是如何避免实例以及关系冲突问题，造成不必要的冗余</li><li>模式层的融合，将新得到的本体融合到已有的本体库中</li></ul><p>合并关系数据库，也是知识图谱构建的重要环节，因为一般来说关系数据库是已经构建好的高质量内部知识源（大部分经过人工干预）。为了将这些结构化的历史数据融入到知识图谱中，可以采用资源描述框架（RDF）作为数据模型。学术界和工业界将数据转换过程称为 RDB2RDF，其实质就是将关系数据库的数据换成RDF的三元组数据。</p><h1 id="知识加工"><a href="#知识加工" class="headerlink" title="知识加工"></a>知识加工</h1><p>通过上面处理，通过信息抽取可以从原始语料库中提取出实体、关系和属性等知识要素，经过知识融合，消除实体指称项与实体对象之间的歧义，得到一系列基本的事实表达。<br>然而事实本身并不等于知识，要想最终获得结构化、网络化的知识体系，还需要经历知识加工的过程。</p><p>知识加工主要涉及3方面内容：本体构建、知识推理和质量评估。</p><h2 id="本体构建"><a href="#本体构建" class="headerlink" title="本体构建"></a>本体构建</h2><p>本体（ontology）是指工人的概念集合、概念框架，如人、事、物等。本体和实体的区别是实体是具象的，本体是抽象的。<br>本体可以采用人工编辑的方式手动构建（借助本体编辑软件），也可以以数据驱动的自动化方式构建本体。因为人工方式构建工作量极大，因此主流的本体库构建，都是从一些面向特定领域的现有本体库出发，采用自动构建技术逐步扩展得到的，是一个迭代过程。</p><p>自动化本体构建过程包含三个阶段：</p><ol><li>实体并列关系相似度计算</li><li>实体上下位关系抽取</li><li>本体的生成</li></ol><p>看下面例子，当知识图谱刚填充“阿里巴巴”、“腾讯”、“手机”这三个实体的时候，一般认为三个之间并没有差别，但当它计算三个实体之间的相似度后，会发现阿里巴巴和腾讯相似度更高。这个例子就是自动化本体构建第一步的作用。</p><p>经过第一步，知识图谱并不能知道阿里巴巴和手机这两个实体根本不属于同一类型。因此就需要实体上下位关系抽取这一步，经过上下位抽取，会生成第三步的本体。</p><p>当三步结束后，知识图谱就能理解，阿里巴巴和腾讯属于公司这个实体下的细分实体，与手机是类目上的不同。</p><p><img src="kg_ontology.jpeg" alt="本体构建过程"></p><h2 id="知识推理"><a href="#知识推理" class="headerlink" title="知识推理"></a>知识推理</h2><p>本体构建完成后，知识图谱具备雏形。这个阶段的图谱，往往关系残缺现象严重，利用知识推理技术，去完成进一步的知识发现。</p><p>我们可以发现：如果A是B的配偶，B是C的主席，C坐落于D，那么我们可以推理：A生活在D这个城市。<br>根据这一规则，就可以去挖掘一下在图谱中，是否还存在其他的path满足这个条件，顺而将AD这两个实体关联起来。<br>除此之外，还可以去思考，串联里有一环，B是C的主席，那么B是C的CEO、B是C的COO等一系列推理策略。</p><p>当然知识推理的对象并不局限于实体间关系，也可以是实体属性值、本体的概念层次关系等。</p><p>推理属性值：已知某实体的生日属性，就可以推理得到年龄属性；<br>推理概念：已知（老虎、科、猫科）和（猫科、目、食肉目），则可以推理出（老虎、目、食肉目）</p><p>这一块的算法主要分为三大类：</p><ul><li>基于逻辑的推理</li><li>基于图的推理</li><li>基于深度学习的推理</li></ul><p><img src="kg_tuili.jpeg" alt="推理算法总结"></p><h2 id="质量评估"><a href="#质量评估" class="headerlink" title="质量评估"></a>质量评估</h2><p>质量评估也是知识库构建技术的重要组成部分，意义在于对知识的置信度进行量化，通过舍弃置信度较低的知识来保证整体知识库的质量。</p><h1 id="知识更新"><a href="#知识更新" class="headerlink" title="知识更新"></a>知识更新</h1><p>从逻辑上看，知识库的更新包括<strong>概念层的更新和数据层的更新</strong>。<br>概念层的更新是指新增数据后获得了新的概念，需要自动将新的概念添加到知识库的概念层中。<br>数据层的更新主要是新增或更新实体、关系、属性值，对数据层进行更新需要考虑数据源的可靠性、数据的一致性（是否存在矛盾或冗余等问题）等可靠数据源，并选择在各数据源中出现频率高的事实和属性加入知识库。</p><p>知识图谱的内容更新有两种方式：</p><ul><li>全面更新：指以更新后的全部数据为输入，从零开始构建知识图谱。方法简单但消耗极大，同时可能需要人工维护；</li><li>增量更新：以新增数据为输入，向现有知识图谱中添加新增知识。方法消耗小但算法复杂度高，也需要大量人工干预（定义规则等）。</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>通过知识图谱，不仅可以将互联网信息表达成更接近人类认知世界的形式，而且提供了一种更好的组织、管理和利用海量信息的方式。目前的知识图谱技术主要用于<strong>智能语义搜索</strong>、<strong>移动个人助理（如Siri）</strong>、<strong>深度问答系统（Watson）</strong>，支撑这些应用的核心技术就是知识图谱技术。</p><p>在智能语义搜索中，当用户发起查询时，搜索引擎会借助知识图谱的帮助对用户查询的关键词进行解析和推理，进而将其映射到知识图谱中的一个或一组概念上，然后解析知识图谱的概念层次结构，向用户返回图形化的知识结构，这就是我们在google和baidu搜索结果中看到的知识卡片。</p><p>在深度问答应用中，系统同样会首先在知识图谱的帮助下对用户使用自然语言提出的问题进行语义分析和语法分析，进而将其转化成结构化形式的查询语句，然后在知识图谱中查询答案。<br>比如，用户提问：“如何判断是否感染了某种病毒？”，则查询会被等价变换为“某种病毒的症状”，然后在进行推理变换，最终形成等价的三元组查询语句，如（病毒、症状、？）和（病毒、征兆、？）等。如果由于知识库不完善而无法通过推理解答用户的问题，深度问答系统会利用搜索引擎向用户反馈搜索结构，同时根据搜索结果更新知识库，从而为回答后续的提问做出准备。</p><p><a href="https://www.cnblogs.com/huangyc/p/10043749.html" target="_blank" rel="noopener">原文链接</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 知识图谱 </tag>
            
            <tag> 信息抽取 </tag>
            
            <tag> 知识融合 </tag>
            
            <tag> 知识加工 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于NLP的关键词提取技术</title>
      <link href="/2021/11/24/nlp-text-key/"/>
      <url>/2021/11/24/nlp-text-key/</url>
      
        <content type="html"><![CDATA[<h2 id="关键词提取"><a href="#关键词提取" class="headerlink" title="关键词提取"></a>关键词提取</h2><p><strong>关键词</strong>是能够表达文档中心内容的词语，常用于计算机系统标引论文内容特征、信息检索、系统汇集以供读者检阅。关键词提取是文本挖掘领域的一个分支，是文本检索、文本比较、摘要生成、文档分类和聚类等文本挖掘研究的基础性工作。<br>从算法角度，关键词提取算法主要有两类：无监督关键词提取算法和有监督关键词提取算法。</p><ol><li>无监督关键词提取算法<br>不需要人工标注的语料，利用某些方法发现文本中比较重要的词作为关键词，进行关键词提取。该方法是先抽取出候选词，然后对每个候选词进行打分，输出topK个分值最高的候选词作为关键词。根据打分的策略不同，有不同的算法，如TF-IDF、TextRank、LDA等算法。<br>无监督关键词提取方法主要有三类：<strong>基于统计特征的关键词提取（TF-IDF）</strong>；<strong>基于词图模型的关键词提取（PageRank，TextRank）</strong>；<strong>基于主题模型的关键词提取（LDA）</strong>。</li></ol><p><img src="total_method.png" alt="无监督算法分类"></p><ul><li>基于统计特征的关键词提取算法思想是利用文档中词语的统计信息抽取文档的关键词；</li><li>基于词图模型的关键词提取首先要构建文档的语言网络图，然后对语言进行网络图分析，在这个图上寻找具有重要作用的词或者短语，这些短语就是文档的关键词；</li><li>基于主题关键词提取算法主要利用的是主题模型中关于主题分布的性质进行关键词提取；</li></ul><ol start="2"><li>有监督关键词提取算法<br>将关键词抽取过程视为二分类问题，先提取出候选词，然后对于每个候选词划定标签，要么是关键词，要么不是关键词，然后训练关键词抽取分类器。当新来一篇文档时，提取出所有的候选词，然后利用训练好的关键词提取分类器，对各个候选词进行分类，最终将标签为关键词的候选词作为关键词。</li></ol><p><img src="supervise.jpeg" alt="有监督算法处理流程"></p><ol start="3"><li><p>两种算法优缺点<br>无监督方法不需要人工标注训练集合的过程，因此更加快捷，但由于无法有效综合利用多信息对候选关键词排序，所以效果无法与有监督方法媲美；而有监督方法可以通过训练学习调节多种信息对于判断关键词的影响程度，因此效果更优，有监督的文本关键词提取算法需要高昂的人工成本（标注），因此现有的文本关键词提取主要采用适用性较强的无监督关键词提取。</p></li><li><p>关键词提取工具/sdk</p></li></ol><ul><li>jieba</li><li><a href="https://pypi.org/project/textrank4zh/0.3/" target="_blank" rel="noopener">Textrank4zh</a>（TextRank算法工具）</li><li><a href="https://pypi.org/project/snownlp/" target="_blank" rel="noopener">SnowNLP</a>（中文分析，简体中文文本处理）</li><li><a href="https://pypi.org/project/textblob/" target="_blank" rel="noopener">TextBlob</a>（英文分析）</li></ul><h2 id="TF-IDF关键词提取算法实现"><a href="#TF-IDF关键词提取算法实现" class="headerlink" title="TF-IDF关键词提取算法实现"></a>TF-IDF关键词提取算法实现</h2><p>TF-IDF算法的详细介绍及实现方法：<a href="https://blog.csdn.net/asialee_bird/article/details/81486700" target="_blank" rel="noopener">算法实现</a></p><h2 id="TextRank关键词提取算法实现"><a href="#TextRank关键词提取算法实现" class="headerlink" title="TextRank关键词提取算法实现"></a>TextRank关键词提取算法实现</h2><p>TextRank算法的详细介绍及实现方法：<a href="https://blog.csdn.net/asialee_bird/article/details/96894533" target="_blank" rel="noopener">算法实现</a></p><h2 id="LDA主题模型关键词提取算法实现"><a href="#LDA主题模型关键词提取算法实现" class="headerlink" title="LDA主题模型关键词提取算法实现"></a>LDA主题模型关键词提取算法实现</h2><ol><li>LDA（Latent Dirichlet Allocation）文档主题生成模型</li></ol><p><strong>主题模型</strong>是一种统计模型，用于发现文档集合中出现的抽象“主题”。主题建模是一种常用的文本挖掘工具，用于在文本体中发现隐藏的语义结构。<br>LDA也称三层贝叶斯概率模型，包含词、主题、文档三层结构；利用文档中单词的共现关系来对单词按主题聚类，得到“文档-主题”和“主题-单词”2个概率分布。<br>详细可以参考<a href="https://blog.csdn.net/v_JULY_v/article/details/41209515" target="_blank" rel="noopener">LDA通俗理解</a></p><ol start="2"><li>基于LDA主题模型的关键词提取算法实现</li></ol><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> gensim <span class="token keyword">import</span> corpora<span class="token punctuation">,</span> models<span class="token keyword">import</span> jieba<span class="token punctuation">.</span>posseg <span class="token keyword">as</span> jp<span class="token keyword">import</span> jieba<span class="token comment" spellcheck="true"># 简单文本处理</span><span class="token keyword">def</span> <span class="token function">get_text</span><span class="token punctuation">(</span>text<span class="token punctuation">)</span><span class="token punctuation">:</span>    flags <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'n'</span><span class="token punctuation">,</span> <span class="token string">'nr'</span><span class="token punctuation">,</span> <span class="token string">'ns'</span><span class="token punctuation">,</span> <span class="token string">'nt'</span><span class="token punctuation">,</span> <span class="token string">'eng'</span><span class="token punctuation">,</span> <span class="token string">'v'</span><span class="token punctuation">,</span> <span class="token string">'d'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 词性</span>    stopwords <span class="token operator">=</span> <span class="token punctuation">(</span><span class="token string">'的'</span><span class="token punctuation">,</span> <span class="token string">'就'</span><span class="token punctuation">,</span> <span class="token string">'是'</span><span class="token punctuation">,</span> <span class="token string">'用'</span><span class="token punctuation">,</span> <span class="token string">'还'</span><span class="token punctuation">,</span> <span class="token string">'在'</span><span class="token punctuation">,</span> <span class="token string">'上'</span><span class="token punctuation">,</span> <span class="token string">'作为'</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 停用词</span>    words_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>    <span class="token keyword">for</span> text <span class="token keyword">in</span> texts<span class="token punctuation">:</span>        words <span class="token operator">=</span> <span class="token punctuation">[</span>w<span class="token punctuation">.</span>word <span class="token keyword">for</span> w <span class="token keyword">in</span> jp<span class="token punctuation">.</span>cut<span class="token punctuation">(</span>text<span class="token punctuation">)</span> <span class="token keyword">if</span> w<span class="token punctuation">.</span>flag <span class="token keyword">in</span> flags <span class="token operator">and</span> w<span class="token punctuation">.</span>word <span class="token operator">not</span> <span class="token keyword">in</span> stopwords<span class="token punctuation">]</span>        words_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>words<span class="token punctuation">)</span>    <span class="token keyword">return</span> words_list<span class="token comment" spellcheck="true"># 生成LDA模型</span><span class="token keyword">def</span> <span class="token function">LDA_model</span><span class="token punctuation">(</span>words_list<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 构造词典</span>    <span class="token comment" spellcheck="true"># Dictionary()方法遍历所有的文本，为每个不重复的单词分配一个单独的整数ID，同时收集该单词出现次数以及相关的统计信息</span>    dictionary <span class="token operator">=</span> corpora<span class="token punctuation">.</span>Dictionary<span class="token punctuation">(</span>words_list<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>dictionary<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'打印查看每个单词的id:'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>dictionary<span class="token punctuation">.</span>token2id<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 打印查看每个单词的id</span>    <span class="token comment" spellcheck="true"># 将dictionary转化为一个词袋</span>    <span class="token comment" spellcheck="true"># doc2bow()方法将dictionary转化为一个词袋。得到的结果corpus是一个向量的列表，向量的个数就是文档数。</span>    <span class="token comment" spellcheck="true"># 在每个文档向量中都包含一系列元组,元组的形式是（单词 ID，词频）</span>    corpus <span class="token operator">=</span> <span class="token punctuation">[</span>dictionary<span class="token punctuation">.</span>doc2bow<span class="token punctuation">(</span>words<span class="token punctuation">)</span> <span class="token keyword">for</span> words <span class="token keyword">in</span> words_list<span class="token punctuation">]</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'输出每个文档的向量:'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>corpus<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 输出每个文档的向量</span>    <span class="token comment" spellcheck="true"># LDA主题模型</span>    <span class="token comment" spellcheck="true"># num_topics -- 必须，要生成的主题个数。</span>    <span class="token comment" spellcheck="true"># id2word    -- 必须，LdaModel类要求我们之前的dictionary把id都映射成为字符串。</span>    <span class="token comment" spellcheck="true"># passes     -- 可选，模型遍历语料库的次数。遍历的次数越多，模型越精确。但是对于非常大的语料库，遍历太多次会花费很长的时间。</span>    lda_model <span class="token operator">=</span> models<span class="token punctuation">.</span>ldamodel<span class="token punctuation">.</span>LdaModel<span class="token punctuation">(</span>corpus<span class="token operator">=</span>corpus<span class="token punctuation">,</span> num_topics<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> id2word<span class="token operator">=</span>dictionary<span class="token punctuation">,</span> passes<span class="token operator">=</span><span class="token number">10</span><span class="token punctuation">)</span>    <span class="token keyword">return</span> lda_model<span class="token keyword">if</span> __name__ <span class="token operator">==</span> <span class="token string">"__main__"</span><span class="token punctuation">:</span>    texts <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token string">'作为千元机中为数不多拥有真全面屏的手机，OPPO K3一经推出，就簇拥不少粉丝'</span><span class="token punctuation">,</span> \             <span class="token string">'很多人在冲着这块屏幕购买了OPPO K3之后，发现原来K3的过人之处不止是在屏幕上'</span><span class="token punctuation">,</span> \             <span class="token string">'OPPO K3的消费者对这部手机总体还是十分满意的'</span><span class="token punctuation">,</span> \             <span class="token string">'吉利博越PRO在7月3日全新吉客智能生态系统GKUI19发布会上正式亮相'</span><span class="token punctuation">,</span> \             <span class="token string">'今年上海车展，长安CS75 PLUS首次亮相'</span><span class="token punctuation">,</span> \             <span class="token string">'普通版车型采用的是双边共双出式排气布局；运动版本车型采用双边共四出的排气布局'</span><span class="token punctuation">]</span>    <span class="token comment" spellcheck="true"># 获取分词后的文本列表</span>    words_list <span class="token operator">=</span> get_text<span class="token punctuation">(</span>texts<span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'分词后的文本：'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>words_list<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 获取训练后的LDA模型</span>    lda_model <span class="token operator">=</span> LDA_model<span class="token punctuation">(</span>words_list<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 可以用 print_topic 和 print_topics 方法来查看主题</span>    <span class="token comment" spellcheck="true"># 打印所有主题，每个主题显示5个词</span>    topic_words <span class="token operator">=</span> lda_model<span class="token punctuation">.</span>print_topics<span class="token punctuation">(</span>num_topics<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> num_words<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'打印所有主题，每个主题显示5个词:'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>topic_words<span class="token punctuation">)</span>    <span class="token comment" spellcheck="true"># 输出该主题的的词及其词的权重</span>    words_list <span class="token operator">=</span> lda_model<span class="token punctuation">.</span>show_topic<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'输出该主题的的词及其词的权重:'</span><span class="token punctuation">)</span>    <span class="token keyword">print</span><span class="token punctuation">(</span>words_list<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可在本机python环境中运行并查看结果。</p><h2 id="Word2Vec词聚类关键词提取算法实现"><a href="#Word2Vec词聚类关键词提取算法实现" class="headerlink" title="Word2Vec词聚类关键词提取算法实现"></a>Word2Vec词聚类关键词提取算法实现</h2><ol><li><p>Word2Vec词向量表示<br>利用浅层神经网络模型自动学习词语在语料库中的出现情况，把词语嵌入到一个高维的空间中，通常在100-500维，在高维空间中词语被表示为词向量的形式。<br>特征词向量的抽取是基于已经训练好的词向量模型。</p></li><li><p>K-means聚类算法<br>聚类算法旨在数据中发现数据对象之间的关系，将数据进行分组，使得组内的相似性尽可能的大，组间的相似性尽可能的小。</p></li></ol><p><strong>算法思想是</strong>：首先随机选择K个点作为初始质心，K为用户指定的所期望的簇的个数，通过计算每个点到各个质心的距离，将每个点指派到最近的质心形成K个簇，然后根据指派到簇的点重新计算每个簇的质心，重复指派和更新质心的操作，直到簇不发生变化或达到最大的迭代次数则停止。</p><ol start="3"><li>基于Word2Vec词聚类关键词提取方法的实现过程</li></ol><p><strong>主要思路</strong>是对于用词向量表示的词语，通过K-means算法对文章中的词进行聚类，选择聚类中心作为文本的一个主要关键词，计算其他词与聚类中心的距离即相似度，选择topK个距离聚类中心最近的词作为关键词，而这个词间相似度可用Word2Vec生成的向量计算得到。</p><p><strong>具体步骤如下：</strong></p><ul><li>对语料进行Word2Vec模型训练，得到词向量文件；</li><li>对文本进行预处理获得N个候选关键词；</li><li>遍历候选关键词，从词向量文件中提取候选关键词的词向量表示；</li><li>对候选关键词进行K-means聚类，得到各个类别的聚类中心（需要人为给定聚类的个数）；</li><li>计算各类别下，组内词语与聚类中心的距离（欧几里得距离或曼哈顿距离），按聚类大小进行降序排序；</li><li>对候选关键词计算结果得到排名前TopK个词语作为文本关键词。</li></ul><p>注：第三方工具包Scikit-learn提供了K-means聚类算法的相关函数，本文用到了sklearn.cluster.KMeans()函数执行K-means算法，sklearn.decomposition.PCA()函数用于数据降维以便绘制图形。</p><h2 id="信息增益关键词提取算法实现"><a href="#信息增益关键词提取算法实现" class="headerlink" title="信息增益关键词提取算法实现"></a>信息增益关键词提取算法实现</h2><p>信息增益算法的详细介绍及实现方法：<a href="https://blog.csdn.net/asialee_bird/article/details/81084783" target="_blank" rel="noopener">算法实现</a></p><h2 id="互信息关键词提取算法实现"><a href="#互信息关键词提取算法实现" class="headerlink" title="互信息关键词提取算法实现"></a>互信息关键词提取算法实现</h2><ol><li><p>互信息（Mutual Information，MI）<br>在概率论和信息论中，两个随机变量的互信息或转移信息（transinformation）是变量间相互依赖性的量度。不同于相关系数，互信息并不局限于实值随机变量，它更加一般且决定着联合分布 $p(X,Y)$ 和分解的边缘分布的乘积 $p(X)p(Y)$ 的相似程度。互信息是度量两个事件集合之间的相关性（mutual dependence）。<br>互信息被广泛用于度量一些语言现象的相关性。在信息论中，互信息被用于衡量两个词的相关度，也用来计算词与类别之间的相关性。</p></li><li><p>互信息计算公式<br>其中 $p(X,Y)$ 是X和Y的联合概率分布函数，而p(X)和p(Y)分别是X和Y的边缘概率分布函数。<br>$$<br>I(X; Y) = \sum_{y\in Y}\sum_{x\in X}p(x, y) \log(\frac{p(x,y)}{p(x)p(y)})<br>$$</p></li></ol><p>$$<br>\begin{aligned}<br>I(X; Y) &amp;= \sum_{x,y}p(x, y) \log{\frac{p(x, y)}{p(x)p(y)}}\\<br>&amp;= \sum_{x,y}p(x, y) \log{\frac{p(x, y)}{p(x)}} - \sum_{x,y}p(x, y) \log p(y)\\<br>&amp;= \sum_{x,y}p(x)p(y|x) \log(py|x) - \sum_{x,y}p(x, y) \log{p(y)}\\<br>&amp;= \sum_xp(x)(\sum_yp(y|x) \log{p(y|x)}) - \sum_y \log{p(y)}(\sum_xp(x,y))\\<br>&amp;= -\sum_xp(x)H(Y|X=x) - \sum_y \log{p(y)p(y)}\\<br>&amp;= -H(Y|X) + H(Y)\\<br>&amp;= H(Y) - H(Y|X)<br>\end{aligned}<br>$$</p><ol start="3"><li>互信息算法实现</li></ol><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn <span class="token keyword">import</span> metrics<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token comment" spellcheck="true"># 训练集和训练标签</span>x_train <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>y_train <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 测试集和测试标签</span>x_test <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>x_train <span class="token operator">=</span> np<span class="token punctuation">.</span>array<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 转为array</span><span class="token comment" spellcheck="true"># 存储每个特征与标签相关性得分</span>features_score_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>x_train<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment" spellcheck="true"># 计算每个特征与标签的互信息</span>    feature_info <span class="token operator">=</span> metrics<span class="token punctuation">.</span>mutual_info_score<span class="token punctuation">(</span>y_train<span class="token punctuation">,</span> x_train<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> i<span class="token punctuation">]</span><span class="token punctuation">)</span>    features_score_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>feature_info<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span>features_score_list<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>请在本机查看运行结果。</p><ol start="4"><li>信息论中的互信息和决策树中的信息增益的关系</li></ol><p>$$<br>\begin{aligned}<br>&amp; 信息论中互信息：\\<br>&amp; I(X;Y) = H(X) - H(X|Y)\\<br>&amp; 决策树中信息增益：\\<br>&amp; G(D,A) = H(D) - H(D|A)<br>\end{aligned}<br>$$</p><p>两者表达意思是一样的，都是表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。</p><p>注：</p><ul><li>标准化互信息（Normalized Mutual Information，NMI）可以用来衡量两种聚类结果的相似度。</li><li>标准化互信息Sklearn实现：metrics.normalized_mutual_info_score(y_train, x_train[:,i])。</li><li>点互信息（Pointwise Mutual Information，PMI）这个指标来衡量两个事物之间的相关性（比如两个词）。</li></ul><h2 id="卡方检验关键词提取算法实现"><a href="#卡方检验关键词提取算法实现" class="headerlink" title="卡方检验关键词提取算法实现"></a>卡方检验关键词提取算法实现</h2><ol><li><p>卡方检验<br>卡方是数理统计中用于检验两个变量独立性的方法，是一种确定两个分类变量之间是否存在相关性的统计方法，经典的卡方检验师检验定性自变量对定性因变量的相关性。</p></li><li><p>基本思路</p></li></ol><ul><li>原假设：两个变量是独立的</li><li>计算实际观察值和理论值之间的偏离程度</li><li>如果偏差足够小，小于设定阈值，就接受原假设；否则就否定原假设，认为两变量是相关的。</li></ul><ol start="3"><li>计算公式</li></ol><p>$$<br>x^2 = \sum \frac{(A-T)^2}{T}<br>$$</p><p>其中，A为实际值，T为理论值。卡方检验可用于文本分类问题中的特征选择，此时不需要设定阈值，只关心找到最为相关的topK个特征。基本思想：比较理论频数和实际频数的吻合程度或者拟合优度问题。</p><ol start="4"><li>基于sklearn的卡方检验实现</li></ol><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> SelectKBest<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> chi2<span class="token comment" spellcheck="true"># 训练集和训练标签</span>x_train <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">,</span>           <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>y_train <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 测试集和测试标签</span>x_test <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">]</span>y_test <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 卡方检验选择特征</span>chi2_model <span class="token operator">=</span> SelectKBest<span class="token punctuation">(</span>chi2<span class="token punctuation">,</span> k<span class="token operator">=</span><span class="token number">3</span><span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 选择k个最佳特征</span><span class="token comment" spellcheck="true"># 该函数选择训练集里的k个特征，并将训练集转化所选特征</span>x_train_chi2 <span class="token operator">=</span> chi2_model<span class="token punctuation">.</span>fit_transform<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将测试集转化为所选特征</span>x_test_chi2 <span class="token operator">=</span> chi2_model<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'各个特征的得分：'</span><span class="token punctuation">,</span> chi2_model<span class="token punctuation">.</span>scores_<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'各个特征的p值：'</span><span class="token punctuation">,</span> chi2_model<span class="token punctuation">.</span>pvalues_<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># p值越小，置信度越高，得分越高</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'所选特征的索引：'</span><span class="token punctuation">,</span> chi2_model<span class="token punctuation">.</span>get_support<span class="token punctuation">(</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'特征提取转换后的训练集和测试集...'</span><span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'x_train_chi2:'</span><span class="token punctuation">,</span> x_train_chi2<span class="token punctuation">)</span><span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'x_test_chi2:'</span><span class="token punctuation">,</span> x_test_chi2<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>请在本机查看运行结果。</p><h2 id="基于树模型关键词提取算法实现"><a href="#基于树模型关键词提取算法实现" class="headerlink" title="基于树模型关键词提取算法实现"></a>基于树模型关键词提取算法实现</h2><ol><li>树模型<br>主要包括决策树和随机森林，基于树的预测模型（sklearn.tree模块和sklearn.ensemble模块）能够用来计算特征的重要程度，因此能用来去除不相关的特征（结合sklearn.feature_selection.SelectFromModel）</li></ol><p>sklearn.ensemble模块包含了两种基于随机决策树的平均算法：<strong>RandomForest</strong>算法和<strong>Extra-Trees</strong>算法。这两种算法都采用了很流行的树设计思想：perturb-and-combine思想。这种方法会在分类器的构建时，通过引入随机化，创建一组各不一样的分类器。这种ensemble方法的预测会给出各个分类器预测的平均。</p><ul><li><p>RandomForests在随机森林(RF)中，该ensemble方法中的每棵树都基于一个通过可放回抽样(boostrap)得到的训练集构建。相反的，在features的子集中随机进行split反倒是最好的split方式。sklearn的随机森林(RF)实现通过对各分类结果预测求平均得到，而非让每个分类器进行投票（vote）。</p></li><li><p>Ext-Trees在Ext-Trees中（详见ExtraTreesClassifier和ExtraTreesRegressor），该方法中，随机性在划分时会更进一步进行计算。在随机森林中，会使用候选feature的一个随机子集，而非查找最好的阈值，对于每个候选feature来说，阈值是抽取的，选择这种随机生成阈值的方式作为划分原则。</p></li></ul><ol start="2"><li>树模型的关键词提取算法实现</li></ol><ul><li>部分代码实现</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>tree <span class="token keyword">import</span> DecisionTreeClassifier<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> RandomForestClassifier<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>ensemble <span class="token keyword">import</span> ExtraTreesClassifier<span class="token keyword">from</span> sklearn<span class="token punctuation">.</span>feature_selection <span class="token keyword">import</span> SelectFromModel<span class="token comment" spellcheck="true"># 导入SelectFromModel结合ExtraTreesClassifier计算特征重要性，并按重要性阈值选择特征。</span><span class="token comment" spellcheck="true"># 基于树模型进行模型选择</span>clf_model <span class="token operator">=</span> ExtraTreesClassifier<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span><span class="token number">250</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>clf_model<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 获取每个词的特征权重,数值越高特征越重要</span>importances <span class="token operator">=</span> clf_model<span class="token punctuation">.</span>feature_importances_<span class="token comment" spellcheck="true"># 选择特征重要性为1.5倍均值的特征</span>model <span class="token operator">=</span> SelectFromModel<span class="token punctuation">(</span>clf_model<span class="token punctuation">,</span> threshold<span class="token operator">=</span><span class="token string">'1.5*mean'</span><span class="token punctuation">,</span> prefit<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>x_train_new <span class="token operator">=</span> model<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>x_train<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 返回训练集所选特征</span>x_test_new <span class="token operator">=</span> model<span class="token punctuation">.</span>transform<span class="token punctuation">(</span>x_test<span class="token punctuation">)</span>  <span class="token comment" spellcheck="true"># 返回测试集所选特征</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li>部分代码实现</li></ul><pre class="line-numbers language-python"><code class="language-python"><span class="token comment" spellcheck="true"># 训练集和训练标签</span>x_train<span class="token punctuation">,</span> y_train<span class="token comment" spellcheck="true"># 候选特征词列表</span>words_list<span class="token comment" spellcheck="true"># 基于树模型进行模型选择</span>forest <span class="token operator">=</span> RandomForestClassifier<span class="token punctuation">(</span>n_estimators<span class="token operator">=</span><span class="token number">250</span><span class="token punctuation">,</span> random_state<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>forest<span class="token punctuation">.</span>fit<span class="token punctuation">(</span>x_train<span class="token punctuation">,</span> y_train<span class="token punctuation">)</span>importances <span class="token operator">=</span> forest<span class="token punctuation">.</span>feature_importances_  <span class="token comment" spellcheck="true"># 获取每个词的特征权重</span><span class="token comment" spellcheck="true"># 将词和词的权重存入字典</span>feature_words_dic <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span><span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>words_list<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    feature_words_dic<span class="token punctuation">[</span>words_list<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token punctuation">]</span> <span class="token operator">=</span> importances<span class="token punctuation">[</span>i<span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 对字典按权重由大到小进行排序</span>words_info_dic_sort <span class="token operator">=</span> sorted<span class="token punctuation">(</span>words_info_dic<span class="token punctuation">.</span>items<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> key<span class="token operator">=</span><span class="token keyword">lambda</span> x<span class="token punctuation">:</span> x<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> reverse<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 将关键词和词的权重分别存入列表</span>keywords_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 关键词列表</span>features_list <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>  <span class="token comment" spellcheck="true"># 关键权重列表</span><span class="token keyword">for</span> word <span class="token keyword">in</span> words_info_dic_sort<span class="token punctuation">:</span>    keywords_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>word<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">)</span>    features_list<span class="token punctuation">.</span>append<span class="token punctuation">(</span>word<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token comment" spellcheck="true"># 选取前一千个关键词和权重写入文本文件</span>keywords <span class="token operator">=</span> keywords_list<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">1000</span><span class="token punctuation">]</span>features <span class="token operator">=</span> features_list<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token number">1000</span><span class="token punctuation">]</span><span class="token comment" spellcheck="true"># 将含有关键字的文本写入文件</span><span class="token keyword">with</span> open<span class="token punctuation">(</span><span class="token string">'data/keywords_features.txt'</span><span class="token punctuation">,</span> <span class="token string">'a'</span><span class="token punctuation">,</span> encoding<span class="token operator">=</span><span class="token string">"utf-8"</span><span class="token punctuation">)</span> <span class="token keyword">as</span> f<span class="token punctuation">:</span>    <span class="token keyword">for</span> i <span class="token keyword">in</span> range<span class="token punctuation">(</span>len<span class="token punctuation">(</span>keywords<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        f<span class="token punctuation">.</span>write<span class="token punctuation">(</span>keywords<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">'\t'</span> <span class="token operator">+</span> features<span class="token punctuation">[</span>i<span class="token punctuation">]</span> <span class="token operator">+</span> <span class="token string">'\n'</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本文总结了本人在实验过程中所用到的常用关键词抽取方法，实验数据是基于公司的内部数据，但此篇总结只是方法上的讲解和实现，没有针对某一具体数据集做相应的结果分析。从实验中可以很明显看出有监督关键词抽取方法通常会显著好于无监督方法，但是有监督方法依赖一定规模的标注数据。</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>广义文本的向量表示方法</title>
      <link href="/2021/11/12/text2vec/"/>
      <url>/2021/11/12/text2vec/</url>
      
        <content type="html"><![CDATA[<p>NLP的基础工作就是文本处理，向量化文本表示的核心目的是将文本表示为一系列的能够表达文本语义的向量，有word2vec、str2vec和doc2vec技术，区别是目标处理单元不同。<br>word2vec即词向量，str2vec为句向量，doc2vec则为文档向量。</p><h2 id="word2vec"><a href="#word2vec" class="headerlink" title="word2vec"></a>word2vec</h2><p>词袋（bag of word）模型是最早的以词为基础处理单元的文本向量化方法。该模型产生的向量与原来文本中单词出现的顺序没有关系，而是词典中每个单词再文本中出现的频率。该方法虽然简单，但有以下问题：</p><ol><li>维度灾难</li><li>无法保留词序信息</li><li>存在语义鸿沟</li></ol><p>随着互联网技术的发展，大量无标注数据的产生，研究中心转移到利用无标注数据挖掘有价值的信息上来。word2vec（词向量）就是为了利用神经网络，从大量无标注的文本中提取有用的信息而产生。</p><p>词袋模型只是将词语符号化，所以词袋模型是不包含语义信息的。如何使词表示包含语义信息是该领域研究者面临的问题。分布假设（distributional hypothesis）的提出为解决上述问题提供了理论基础。该假设的核心思想是：上下文相似的词，其语义也相似。随后大量研究人员整理了利用上下文表示词义的方法，这类方法就是有名的词空间模型（word space model）。通过语言模型构建上下文与目标词之间的关系，是一种常见的方法，神经网络词向量模型就是根据上下文与目标词之间的关系进行建模。</p><h2 id="NNLM"><a href="#NNLM" class="headerlink" title="NNLM"></a>NNLM</h2><p>神经网络语言模型（Neural Network Language Model，NNLM）与传统方法估算的不同在于通过一个神经网络结构对n元条件概率进行估计。由于NNLM模型使用低维紧凑的词向量对上下文进行表示，解决了词袋模型带来的数据稀疏、语义鸿沟等问题。另一方面，在相似的上下文环境中，NNLM模型可以预测出相似的目标词，而传统模型无法做到这一点。例如，如果在预料中 A=“小狗在院子里趴着” 出现1000次， B=“小猫在院子里趴着” 出现1次。A和B的唯一区别就是狗和猫，两个词无论在语义还是语法上都相似。根据频率来估算概率P(A) &gt;&gt; P(B)，这显然不合理。如果采用NNLM计算P(A)~P(B)，因为NNLM模型采用低维的向量表示词语，假定相似的词，则其词向量也相似。</p><p><img src="str_nnlm.png" alt="NNLM词向量处理模型"></p><h2 id="C-amp-W"><a href="#C-amp-W" class="headerlink" title="C&amp;W"></a>C&amp;W</h2><p>C&amp;W = context &amp; word(上下文和目标词)，主要目的并不在于生成一份好的词向量，甚至不想训练语言模型，而是要用这份词向量去完成NLP里面的各项任务，比如词性标注、命名实体识别、短语识别、语义角色标注等等。NNLM模型的目标是构建一个语言概率模型，而C&amp;W则是以生成词向量为目标的模型。C&amp;W模型并没有采用语言模型去求解词语上下文的条件概率，而是直接对n元短语打分，这就省去了NNLM模型中从隐藏层到输出层的权重计算，大大降低了运算量。其核心机理是：如果n元短语在语料库中出现过，那么模型会给该短语打高分；如果是未出现则在语料库中的短语则会得到较低的评分。</p><h2 id="CBOW-and-Skip-gram"><a href="#CBOW-and-Skip-gram" class="headerlink" title="CBOW and Skip-gram"></a>CBOW and Skip-gram</h2><p>为了更高效的获取词向量，研究者在NNLM和C&amp;W模型的基础上保留其核心部分，得到CBOW（Continuous Bag of Words）模型和Skip-gram模型。</p><p><img src="cbow.png" alt="CBOW模型"></p><p>模型使用一段文本的中间词作为目标词，去掉了隐藏层，大幅度提升了计算频率。此外，CBOW模型还使用上下文歌词的词向量的平均值替代NNLM模型各个拼接的词向量。即根据上下文来预测当前词语的频率，且上下文所有词出现频率的影响权重是一样的。Skip-gram模型同样没有隐藏层，与CBOW模型输入上下文词的平均词向量不同，它是从目标词W的上下文中选择一个词，将其词向量组成上下文的表示。即根据当前词语来预测上下文概率。</p><h2 id="doc2vec-str2vec"><a href="#doc2vec-str2vec" class="headerlink" title="doc2vec/str2vec"></a>doc2vec/str2vec</h2><p>利用word2vec计算词语间的相似度有非常好的效果，word2vec技术也可以用于计算句子或者其他长文本间的相似度，其一般做法是对文本粉刺后，提取其关键词，用词向量表示这些关键词，接着对关键词向量求平均或者将其拼接，最后利用词向量计算文本间的相似度。这种方法丢失了文本中的次序信息，而文本的语序包含重要信息。为此，有研究者在word2vec的基础上提出了文档向量化（doc2vec），又称str2vec和para2vec。</p><p>doc2vec技术存在两种模型–Distributed Memory(DM)和Distributed Bag of Words(DBOW)，分别对应word2vec技术里的CBOW和Skip-gram模型。Doc2vec相对于word2vec不同之处在于，在输入层，增添了一个新句子向量Paragraph vector， Paragraph vector可以被看作是另一个词向量，它扮演了一个记忆，词袋模型中，因为每次训练只会截取句子中一小部分词训练，而忽略了除了本次训练词以外该句子中的其他词，忽略了文本的词序问题。而doc2vec中的paragraph vector则弥补了这方面的不足，它每次训练也是滑动截取句子中一小部分词来训练，paragraph vector在同一个句子的若干次训练中是共享的，所以同一句话会有多次训练，每次训练中输入都包含paragraph vector。它可以被看作是句子的主旨（中心思想），有了它，该句子的主旨每次都会被放入作为输入的一部分来训练。这样每次训练中，不光是训练了词，得到了词向量。同时随着一句话每次滑动截取若干词训练的过程中，作为每次训练的输入层一部分的共享paragraph vector，该向量的主旨会越来越准确。</p><p><img src="mod_dbow.png" alt="DBOW模型示意图"></p><p>那么doc2vec是怎么预测新的句子paragraph vector呢？其实在预测新句子的时候，还是会将该paragraph vector随机初始化，放入模型中再重新根据随机梯度下降不断迭代求得最终稳定下来的句子向量。不过在预测过程中，模型里的词向量还有投影层到输出层的softmax weights参数是不会变的，这样在不断迭代中只会更新paragraph vector，其他参数均已固定，只需很少的时间就能计算出带预测的paragraph vector。</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 内容推荐 </tag>
            
            <tag> NLP </tag>
            
            <tag> 文本向量化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于广义内容的推荐算法系统</title>
      <link href="/2021/11/04/context-recalg/"/>
      <url>/2021/11/04/context-recalg/</url>
      
        <content type="html"><![CDATA[<h1 id="写在前面-什么是基于内容推荐算法"><a href="#写在前面-什么是基于内容推荐算法" class="headerlink" title="写在前面 - 什么是基于内容推荐算法"></a>写在前面 - 什么是基于内容推荐算法</h1><p>广义内容，是指包括文本、图片、音频、视频等在内的多媒体信息。<br>基于内容的推荐算法(Content-Based Recommendations)是基于标的物相关信息、用户相关信息及用户对标的物的操作行为来构建推荐算法模型，为用户提供推荐服务。这里的标的物相关信息可以是对标的物文字描述的metadata信息、标签、用户评论、人工标注的信息等。用户相关信息是指人口统计学信息(如年龄、性别、偏好、地域、收入等等)。用户对标的物的操作行为可以是评论、收藏、点赞、观看、浏览、点击、加购物车、购买等。基于内容的推荐算法一般只依赖于用户自身的行为为用户提供推荐，不涉及到其他用户的行为。</p><p>广义的标的物相关信息不限于文本信息，图片、语音、视频等都可以作为内容推荐的信息来源，只不过这类信息处理成本较大，不光是算法难度大、处理的时间及存储成本也相对更高。</p><p>基于内容的推荐算法算是最早应用于工程实践的推荐算法，有大量的应用案例，如今日头条的推荐有很大比例是基于内容的推荐算法。</p><h1 id="基于内容的推荐算法实现原理"><a href="#基于内容的推荐算法实现原理" class="headerlink" title="基于内容的推荐算法实现原理"></a>基于内容的推荐算法实现原理</h1><p>基于内容的推荐算法的基本原理是根据用户的历史行为，获得用户的兴趣偏好，为用户推荐跟他的兴趣偏好相似的标的物，下图展示了基于内容的推荐算法逻辑过程。<br><img src="alg_resis.jpeg" alt="算法逻辑"></p><p>从上图也可以看出，要做基于内容的个性化推荐，一般需要三个步骤，它们分别是：基于用户信息及用户操作行为构建用户特征表示、基于标的物信息构建标的物特征表示、基于用户及标的物特征表示为用户推荐标的物。<br><img src="alg_struc.jpeg" alt="核心步骤"></p><p>本节我们先介绍如何基于核心步骤的1和2喂用户做推荐（即达到步骤3），然后分别对这三个步骤加以说明，介绍每个步骤都有哪些方法和策略可供选择。</p><h2 id="基于用户和标的物特征为用户推荐的核心思想"><a href="#基于用户和标的物特征为用户推荐的核心思想" class="headerlink" title="基于用户和标的物特征为用户推荐的核心思想"></a>基于用户和标的物特征为用户推荐的核心思想</h2><p>有了用户特征和标的物特征，推荐思路总结有三个：</p><p><strong>1.基于用户历史行为记录了做推荐</strong><br>我们需要事先计算标的物之间的相似性，然后将用户历史记录中的标的物的相似标的物推荐给用户。<br>不管标的物包含哪类信息，一般的思路是将标的物特征转化为向量化表示，有了向量化表示，就可以通过cosin余弦计算两个标的物向量之间的相似度，余弦是相邻边比值，当比值近似1的情况，相似度最好。</p><p><strong>2.用户和标的物特征都用显式的标签表示，利用该表示做推荐</strong><br>标的物用标签来表示，反过来，每个标签就可以关联一组标的物，那么根据用户的标签表示，用户的兴趣标签就可以关联到一组标的物，这组通过标签关联到的标的物，就可以作为给用户推荐的候选集合。这类方法就是所谓的倒排索引，是搜索业务通用的解决方案。</p><p><strong>3.用户和标的物嵌入到同一个向量空间，基于向量相似（即计算向量余弦值）做推荐</strong><br>当用户和标的物嵌入到同一个向量空间后，就可以通过计算用户和标的物之间的相似度，然后按照标的物跟用户的相似度，为用户推荐相似度高的标的物。还可以基于用户向量表示计算用户相似度，将相似用户喜欢的标的物推荐给该用户，这时标的物的嵌入是不必要的。</p><p>上面就是基于内容推荐的核心思想，那么如何构建用户表示特征向量和标的物表示特征向量就是推荐系统的关键。</p><h2 id="构建用户特征向量"><a href="#构建用户特征向量" class="headerlink" title="构建用户特征向量"></a>构建用户特征向量</h2><p>用户的特征总结来讲有如下方法构建</p><ul><li>基于用户对标的物的操作行为，如点击/购买/收藏/播放/评论等构建用户对标的物的偏好画像；</li><li>基于用户自身的人口统计学特征表示。</li></ul><p>具体来说：<br><strong>1.用户行为记录作为显示特征</strong><br>比如用户浏览过A、B、C三个视频，同时根据每个视频用户观看时长占分别视频总时长的比例给用户行为打分，这是用户的兴趣偏好就可以记录为{$(A, S1), (B, S2), (C, S3)$}，其中S1, S2, S3分别是用户对视频A、B、C的评分。</p><p>该方案直接将用户历史操作过的标的物作为用户特征表示，在推荐时可以将与用户操作过的标的物相似的标的物推荐给用高糊。</p><p><strong>2.显示的标签特征</strong><br>如果标的物是有标签来描述，那么这些标签可以用来表征标的物。用户的兴趣画像也可以基于用户对标的物的行为来打上对应的标签。拿视频推荐来举例，如果用户过去看了科幻和恐怖类电影，那么恐怖和科幻就是用户的偏好标签了。<br>每个标的物的标签可以是包含权重的，而用户对标的物的操作行为也是有权重的，而用户对标的物的操作行为也是有权重的，从而用户的兴趣标签是有权重的。<br>在具体推荐时，可以将用户的兴趣标签关联到的标的物(具备该标签的标的物)推荐给用户。</p><p><strong>3.向量方法的兴趣特征</strong><br>可以基于标的物的信息将标的物嵌入到向量空间中，利用向量来表示标的物，稍后讲解嵌入的算法实现方案。有了标的物的向量化表示，用户的兴趣向量就可以用他操作过的标的物的向量的平均向量来表示了。<br>这里表示用户兴趣向量有多种策略，可以基于用户对操作过的标的物的评分以及时间加权来获取用户的加权偏好向量，而不是直接取平均。另外，我们也可以根据用户操作过的标的物之间的相似度，喂用户构建多个兴趣向量（比如对标的物聚类，用户在某一类上操作过的标的物的向量均值作为用户在这个类别上的兴趣向量），从而更好的表达用户多方位的兴趣偏好。<br>有了用户的兴趣向量及标的物的兴趣向量，可以基于向量相似性计算用户对标的物的偏好度，再基于偏好度大小来为用户推荐标的物。</p><p><strong>4.通过交互方式获取用户兴趣标签</strong><br>在用户首次注册让用户选择自己的兴趣偏好标签，一旦用户勾选了自己的兴趣标签，那么这些兴趣标签就是系统为用户推荐标的物的逻辑基础，具体推荐策略逻辑就可以按照上述方法进行。</p><p><strong>5.用户的人口统计学特征</strong><br>用户在登陆、注册时提供的关于自身相关的信息、通过运营活动用户填写的信息、通过用户行为利用算法逻辑得出的结论，如年龄、性别、地域、收入、爱好、居住地、工作地点等是非常主要的信息。基于这些关于用户维度的信息，我们可以将用户特征用向量化表示出来，向量的维度就是可获取的用户特征数。<br>有了用户特征向量就可以计算用户相似度，将相思用户喜欢的标的物推荐给用户。</p><h2 id="构建标的物特征表示"><a href="#构建标的物特征表示" class="headerlink" title="构建标的物特征表示"></a>构建标的物特征表示</h2><p>标的物的特征，一般可以利用显示的标签来表示，也可以利用隐式的向量（当然one-hot编码也是向量表示，但是不是隐式的）来刻画，向量的每个维度就是一个隐式的特征项。前面提到某些推荐算法需要计算标的物之间的相似度，下面我们在讲标的物的各种特征表示时，也简单介绍一下标的物之间的相似度计算方法。顺便说一下，标的物关联标的物的推荐方式也需要知道标的物之间的相似度。下面从四个方面来详细讲解怎么构建标的物的特征表示。</p><p><strong>1.标的物包含标签信息</strong><br>最简单的方式是将标签按照某种顺序排列，每个标签看成一个纬度，那么每个标的物就可以表示成一个N维向量，如 {item， [tag1, tag2, tag3, …, tagN]}，如果标的物包含某个标签，向量在相应标签上的分量值为1，否则为0，即所谓的one-hot编码。通常N可能非常大，比如视频领域，N可能是几万或者几十万上百万，这时向量是稀疏矩阵，因为一般标的物只有几个或者几十个标签，采用稀疏向量的表示来优化向量存储和计算，提升效率。有个标的物基于标签的向量化表示，很容易基于cosin余弦计算相似度了。</p><p>实际上标签不是这么简单的，有很多业务标签是分级的，比如电商（像淘宝），有多级标签，标签的层级关系形成一颗树状结构，这时向量化有个简单的表示方案，即只考虑最低层级标签（叶节点），基于叶节点标签构建向量表示。更复杂的方法，可以基于层级结构构建标签表示及计算标的物相似度。<br><img src="tag_tree.png" alt="标签树"></p><p>标签是可以通过算法获取的，比如通过NLP从文本信息中提取关键词作为标签。对于图片/视频，通过他们的描述信息提取标签，另外可以通过目标检测的方法从图片/视频中提取相关对象构建标签。</p><p>标签可以是用户打的，很多产品在用户与标的物交互时互相可以为标的物打标签，这些标签就是标的物的一种刻画。标签也可以是人工标注，Netflix在做推荐时，就请了上万专家对视频从上千个纬度来打标签，质量很高。很多行业的标的物来源于第三方供应商，他们在入驻平台时会被要求按照某些规范填写相关标签信息。</p><p><strong>2.标的物具备结构化的信息</strong><br>有些行业标的物是具备结构化信息的，如视频行业，一般会有媒体资料库，库中对每个节目都会有标题、演职员、导演、标签、评分、地域等维度数据，这类数据一般存在传统数据库中。这类数据，可以将一个字段（也是一个特征）作为向量的一个纬度，这时向量化表示每个维度的值不一定是数值，但是形式还是向量化的形式，即所谓的向量空间模型（Vector Space Model，简称VSM）。这时可以通过如下的方式计算两个标的物之间的相似度。<br>假设两个标的物的向量表示分别为<br>$V1=(p_1, p_2, p_3, ……, p_k)$<br>$V2=(q_1, q_2, q_3, ……, q_k)$</p><p>这时两个标的物的相似性计算公式为 $$sim(V_1, V_2)=\sum_{t=1}^k ksim(p_t, q_t)$$</p><p>其中代表的是向量的两个分量之间的相似度。可以采用Jacard相似度等各种方法计算两个分量之间的相似度。上面公式中还可以针对不同的分量采用不同的权重策略，见下面公式，其中第t个分量（特征）的权重，具体权重的数值可以根据对业务的理解来人工设置，或者利用机器学习算法来训练学习得到。<br>$$sim(V_1, V_2)=\sum_{t=1}^k w_t*sim(p_t, q_t)$$</p><p><strong>3.包含文本信息的标的物的特征表示</strong><br>像头条等新闻资讯或搜索类app，标的物就是一篇篇文章（其中会包含图片或视频），文本信息是最重要的信息形式，构建标的物之间的相似性有很多种方法。下面对常用的方法做一些讲解说明。</p><p>a. 利用TF-IDF将文本信息转化为特征向量<br>TF-IDF通过将所有文档（即标的物）粉刺，获得所有不同词的集合（假设有M个词），那么就可以为每个文档构建一个M维（每个词就是一个维度向量）的向量，而该向量中某个词所在维度的值可以通过统计每个词在文档中的重要性来衡量，这个重要性的度量就是TF-IDF。<br>TF是某个词在某篇文章中出现的频次，用于衡量这个词在文档中的重要性，出现次数越多的词重要性越高，副词（的，地）和语气助词等会去掉度量，这些词对构建向量是没有任何实际价值的。TF具体计算公式如下<br>$$<br>TF(t_k, d_j)=\frac{||t_k\in d_j||}{||d_j||}<br>$$<br>其中分子tk是第k个词在文档中出现的次数，分母是dj中词的总个数。</p><p>IDF代表的是某个词在所有文档中的“区分度”，如果某个词只在少量几个文档中出现，那么它包含的价值就是巨大的，如果某个词在很多文档中出现，那么它就不能很好的度量这个文档。下面是IDF的计算公式，其中N是所有文档的个数，是包含次的文档个数，这个公式与前面描述一致，稀有词的区分度大。<br>$$<br>IDF(t_k)=\log \frac{N}{n_k}<br>$$<br>有了上面对TF和IDF的定义，实际的TF-IDF就是上面两个量的乘积：<br>$$<br>TF-IDF(t_k, d_j) = TF(t_k, d_j) * IDF(t_k)<br>$$<br>有了基于TF-IDF计算的标的物（即文档表示为TF-IDF向量）的向量表示，就很容易计算两个标的物（文档）的相似度了（cosin余弦相似度）</p><p>b. 利用LDA算法构建文档（标的物）的主题<br>LDA算法是一类文档主题生成模型，包含词、主题、文档三层结构，是一个三层的贝叶斯概率模型。对于语料库中的每篇文档，LDA定义了如下生成过程：</p><ol><li>对于每篇文档，从主题分布中抽取一个主题；</li><li>从上述被抽取到的主题所对应的单词分布中抽取一个单词；</li><li>重复上述过程直至遍历文档中的每一个单词。</li></ol><p>我们通过对所有文档进行LDA训练，就可以构建每篇文档的主题分布，从而构建一个 <em>基于主题的向量（每个主题就是向量的一个分量，而值就是该主题的概率值）</em> ，这样我们就可以利用该向量来计算两篇文档的相似度了。主题模型可以理解为一个降维过程，将文档的词向量表示将维成主题的向量表示（主题的个数是远远小于词的个数的，所以是降维）。</p><p>c. 利用doc2vec算法构建文本相似度<br>doc2vec或者叫做paragraph2vec, sentence embeddings，是一种非监督式算法，可以获得句子、段落、文章的稠密向量表达，它是word2vec的拓展，2014年被Google的两位大牛提出，并广泛应用于文本分类和情感分析中。通过doc2vec学习出句子、段落、文章的向量表示，可以通过计算向量之间距离来表达句子、段落、文章之间的相似性。</p><p>这里我们简单描述一下doc2vec的核心思想。doc2vec受word2vec启发，由它推广而来，word2vec的设计思路是通过学习一个唯一的向量表示每个词，每个词向量作为矩阵W中的一列（W是所有词向量构成的矩阵），矩阵列可以通过词汇表为每个词做索引，排在索引第一位的放到矩阵W的第一列，如此类推。将学习问题转化为通过上下文词序列中前几个词来预测下一个词。具体的模型框架如下图：<br><img src="alg_word2vec.png" alt="word2vec算法框架"></p><p>简单来说，给定一个待训练的词序列，词向量模型通过极大化平均对数概率<br>$$<br>\frac{1}{T}\sum_{t=k}^{T-k} \log p(w_t|w_{t-k}, …, w_{t+k})<br>$$</p><p>将预测任务通过softmax变换看成一个多分类问题</p><p>$$<br>p(w_t|w_{t-k}, …, w_{t+k}) = \frac{e^{y_{w_t}}}{\sum _ie^{y_i}}<br>$$</p><p>上面公式中词i的归一化对数概率，可以用下面公式计算，其中U、b是参数，h是通过词向量的拼接或者平均来构建的。</p><p>$$<br>y = b + Uh(w_{t-k}, …, w_{t+k};W)<br>$$</p><p>word2vec算法随机初始化词向量，通过随机梯度下降法来训练神经网络模型，最终得到每个词的向量表示。</p><p>doc2vec类似，每个段落/文档表示为向量，作为矩阵D的一列，每个词也表示为一个向量，作为矩阵W的一列。将学习问题转化为通过上下文词序列中前几个词和段落/文档来预测下一个词。将段落/文档和词向量通过拼接或者平均来预测句子的下一个词（下图是通过“the”、“cat”、“sat”及段落id来预测下一个词“on“）。在训练的时候我们固定上下文的长度，用滑动窗口的方法产生训练集。段落向量/句向量在上下文中共享。<br><img src="alg_doc2vec.jpeg" alt="doc2vec模型结构"></p><p>工程上有很多开源框架有word2vec和doc2vec的实现，比如gensim中就有很好的实现。<a href="https://radimrehurek.com/gensim/models/doc2vec.html" target="_blank" rel="noopener">参考链接</a>。</p><p><strong>4.图片、音频、或者视频信息</strong><br>如果标的物包含的是图片、音频或者视频信息，处理会更加复杂。一种方法是利用它们的文本信息（标题、评论、描述信息、利用图像技术提取的字幕文本信息、利用语音识别获取的文本信息等）采用（3）的技术方案获得向量化表示。对于图像或者视频，也可以利用opencv中的PSNR和SSIM算法来表示视频特征，也可以计算视频之间的相似度。总之，图片、图像、音频都可以转化为NLP问题或者图像处理问题，通过图像处理和NLP获得对应的特征表示，从而最终计算出相似度，如下图表示。<br><img src="alg_va.jpeg" alt="视频/图片问题转化为NLP或图像处理问题"></p><h2 id="为用户做个性化推荐"><a href="#为用户做个性化推荐" class="headerlink" title="为用户做个性化推荐"></a>为用户做个性化推荐</h2><p>有了上面用户和标的物的特征表示，剩下就是基于此为用户做个性化推荐了，总结了下面5种方法和策略。这里总结的推荐是完全个性化范式的推荐，为每个用户生成不一样的推荐结果。</p><p><strong>1.采用跟基于物品的协同过滤类似的方式推荐</strong><br>该方法采用基于用户行为记录的显示特征表示用户特征，通过将用户操作过的标的物最相似的标的物推荐给用户，算法原理跟基于物品的协同过滤类似，计算公式甚至是一样的，但是这里计算标的物相似度是基于标的物的自身信息来计算的，而基于物品的协同过滤是基于用户对标的物的行为矩阵来计算的。</p><p>用户u对标的物s的喜好度 sim(u, s) 可以采用如下公式计算，其中U是所有用户操作过的标的物列表，是用户u对标的物的喜好度，是标的物与s的相似度。</p><p>$$<br>sim(u, s) = \sum_{s_i \in U} score(s_i) * sim(s_i, s)<br>$$</p><p>有了用户对每个标的物的相似度，基于相似度降序排列，就可以取topN推荐给用户了。<br>除了采用上面的公式外，我们在推荐时也可以稍作变化，采用最近邻方法（K-NearestNeighbor, KNN）。对于用户操作/喜欢过的每个标的物，通过KNN找到最相似的k个标的物</p><p>$$<br>Rec(u) = \sum_{s_i \in U} {s_j|s_j \in kNN(s_i)}<br>$$</p><p>其中Rec(u)是给用户u的推荐，是标的物最近邻（最相似）的k个标的物。</p><p><strong>2.采用跟基于用户协同过滤类似的方法计算推荐</strong><br>如果我们获得了用户的人口统计学向量表示或者基于用户历史操作行为获得了用户的向量化表示，那么我们可以采用跟基于用户的协同过滤方法相似的方法来为用户提供个性化推荐，具体思路如下：<br>我们可以将与该用户最相似的用户喜欢的标的物推荐给该用户，算法原理跟基于用户的协同过滤类似，计算公式甚至是一样的。但是这里计算用户相似度是基于用户的人口统计学特征向量表示来计算的（计算用户向量cosine余弦相似度）或者是基于用户历史行为嵌入获得的特征向量来计算的，而基于用户的协同过滤是基于用户对标的物的行为矩阵来计算用户之间的相似度。</p><p>用户u对标的物s的喜好度 sim(u, s) 采用如下公式计算，其中U是与该用户最相似的用户集合，是用户对标的物s的喜好度，是用户与该用户u的相似度。</p><p>$$<br>sim(u, s) = \sum_{u_i \in U} sim(u, u_i) * score(u_i, s)<br>$$</p><p>有了用户对每个标的物的相似度，基于相似度降序排列，就可以取topN推荐给用户了。</p><p>与前面一样我们也可以采用最近邻方法(K-NearesNeighbor, KNN)。通过KNN找到最相似的k个用户，将这些用户操作/喜欢过的每个标的物推荐给用户。</p><p>$$<br>Rec(u) = \sum_{u_i \in KNN(u)} {s_j \in A(u_i)}<br>$$</p><p>其中Rec(u)是给用户u的推荐， kNN(u)是用户相似的k个用户。是用户操作/喜欢过的标的物的集合。</p><p><strong>3.基于标的物聚类的推荐</strong><br>有了标的物的向量表示，我们可以用kmeans等聚类算法将标的物聚类，有了标的物的聚类，推荐就好办了。从用户历史行为中的标的物所在的类别挑选用户没有操作行为的标的物推荐给用户，这种推荐方式是非常直观的。电视猫的个性化推荐就采用了类似的思路。具体计算公式如下，其中是给用户u的推荐，H是用户的历史操作行为集合，Cluster(s)是标的物s所在的聚类。</p><p>$$<br>Rec(u) = \sum_{s\in H}{t\in Cluster(s) \&amp; t\neq s}<br>$$</p><p><strong>4.基于向量相似的推荐</strong><br>不管是前面提到的用户的显示兴趣特征（利用标签来衡量用户兴趣）或者是向量式的兴趣特征（将用户的兴趣投影到向量空间），我们都可以获得用户兴趣的向量表示。</p><p>如果我们获得了用户的向量表示和标的物向量表示，那么我们就可以通过向量的cosine余弦相似度计算用户与标的物之间的相似度。一样的，有了用户对每个标的物的相似度，基于相似度降序排列，就可以去topN推荐给用户了。</p><p>基于向量的相似的推荐，需要计算用户向量与每个标的物向量的相似性。如果标的物数量较多，整个计算过程相当耗时。同样，计算标的物最相似的K个标的物，也会涉及到与每个其他标的物计算相似度，也非常耗时。真个计算过程的时间复杂度是O(N)，其中N是标的物的总个数。</p><p>上述复杂的计算过程可以利用Spark等分布式计算平台来加速。对于T+1级（每天更新一次推荐结果）的推荐服务，利用Spark提前计算好，将推荐结果存储起来供前端业务调用。</p><p>另外一种可行的策略是利用高效的向量检索库，在极短时间（毫秒级）内为用户所引出topN最相似的标的物。目前facebook开源的<a href="https://github.com/facebookresearch/faiss" target="_blank" rel="noopener">FAISS库</a>就是一个高效的向量搜索与聚类库，可以在毫秒级响应查询及聚类需求，因此可以用于个性化的实时推荐。目前FAISS已经在推荐业务上得到广泛应用。</p><p><a href="https://github.com/facebookresearch/faiss" target="_blank" rel="noopener">FAISS库</a>适合稠密向量的检索和聚类，所以对于利用LDA、doc2vec算法构建向量表示的方案是实用的，因为这些方法构建的是稠密向量。而对于TF-IDF及基于标签构建的向量化，就不适用了，这两类方法构建的都是稀疏高位矩阵。</p><p><strong>5.基于标签的反向倒排索引做推荐</strong><br>基于标的物的标签和用户的历史兴趣，可以构建出用户基于标签兴趣的画像及推荐与标的物的倒排索引查询表。基于该反向索引表及用户的兴趣画像，我们就可以为用户做个性化推荐了。该类算法其实就是基于标签的召回算法。</p><p>具体推荐过程见下图（基于倒排索引的电影推荐）：从用户画像中获取用户兴趣标签，基于用户的兴趣标签从倒排索引中获取该标签对应的标的物，这样就可以从用户关联到标的物了。其中用户的每个兴趣标签及标签关联到的标的物都是有权重的。</p><p><img src="alg_antiindex.jpeg" alt="基于倒排索引的电影推荐"></p><p>假设用户的兴趣标签及对应的标签权重形如 $\{(T_1, S_1), (T_2, S_2), (T_3, S_3), ……, (T_k, S_k)\}$ ，其中T是标签， S是用户对标签的偏好权重。<br>假设标签关联的标的物分别为</p><p>$$<br>T_1 \longleftrightarrow \{(O_{11}, W_{11}), (O_{12}, W_{12}), (O_{13}, W_{13}), ……, (O_{1p_1}, W_{1p_1})\}<br>$$</p><p>$$<br>T_2 \longleftrightarrow \{(O_{21}, W_{21}), (O_{22}, W_{22}), (O_{23}, W_{23}), ……, (O_{2p_2}, W_{2p_2})\}<br>$$</p><p>$$<br>\cdots<br>$$</p><p>$$<br>T_k \longleftrightarrow \{(O_{k1}, W_{k1}), (O_{k2}, W_{k2}), (O_{k3}, W_{k3}), ……, (O_{kp_k}, W_{kp_k})\}<br>$$ </p><p>其中O、K分别是标的物及对应的权重，那么</p><p>$$<br>U=\sum_{i=1}^k S_i*T_i \<br>$$</p><p>$$<br>=\sum_{i=1}^k S_i*\{(O_{i1}, W_{i1}), (O_{i2}, W_{i2}), (O_{i3}, W_{i3}), …, (O_{ip_i}, W_{ip_i})\}<br>$$</p><p>$$<br>=\sum_{i=1}^k\sum_{j=1}^{p_i} S_i * W_{ij} * O_{ij}<br>$$</p><p>上式中U是用户对标的物的偏好集合，这里将标的物看成向量空间的基，所以有上面公式。不同的标签可以关联到相同的标的物（因为不同的标的物可以有相同的标签），上式中最后一个等号右边需要合并同类项，将相同基前面的系数相加。合并同类项后，标的物（基）前面的数值就是用户对该标的物的偏好程度了，我们对这些偏好程度降序排列，就可以为用户做topN推荐了。</p><p><em>以上就是基于内容的推荐算法的核心原理。</em></p><h1 id="基于内容的推荐算法应用场景"><a href="#基于内容的推荐算法应用场景" class="headerlink" title="基于内容的推荐算法应用场景"></a>基于内容的推荐算法应用场景</h1><h2 id="完全个性化推荐"><a href="#完全个性化推荐" class="headerlink" title="完全个性化推荐"></a>完全个性化推荐</h2><p>为每个用户生成不同的推荐结果。</p><h2 id="标的物关联标的物推荐"><a href="#标的物关联标的物推荐" class="headerlink" title="标的物关联标的物推荐"></a>标的物关联标的物推荐</h2><p>即根据当前用户浏览标的物，做相关标的物列表topN推荐。</p><h2 id="配合其他推荐算法"><a href="#配合其他推荐算法" class="headerlink" title="配合其他推荐算法"></a>配合其他推荐算法</h2><p>由于基于内容的推荐算法在精准度上不如协同过滤肃反啊，但是可以更好的适应冷启动，所以在实际业务中基于内容的推荐算法会配合其他算法一起服务于用户，常用方法是采用级联方式，先给用户协同过滤的推荐结果，如果该用户行为较少导致没有协同过滤推荐结果，就为该用户推荐基于内容的推荐算法产生推荐结果。</p><h2 id="主题推荐"><a href="#主题推荐" class="headerlink" title="主题推荐"></a>主题推荐</h2><p>如果我们有标的物的推荐信息，并且基于推荐系统构建了一套推荐算法，那么我们就可以将用户喜欢的标签采用主题的方式推荐给用户，每个主题就是用户的一个兴趣标签。通过一系列主题罗列展示，让用户从中筛选自己感兴趣的内容。下图呈现了Netflix首页的示例。<br><img src="alg_netflix.jpeg" alt="Netflix首页推荐"></p><p>Netflix的首页大量采用基于主题的推荐模式。主题推荐的好处是可以将用户所有的兴趣点按照兴趣偏好大小先后展示出来，可解释性强，并且让用户有更多维度的自由选择空间。</p><p>在真实产品中可以采用比netflix这个示例首页更好的方式。具体来说，可以为每个标签通过人工编辑生成一句更有表达空间的话（如武侠标签，可以采用“江湖风云再起，各大门派齐聚论剑”这样更有深度的表述），具体前端展示映射到人工填充的话而不是直接展示原来的标签。</p><h2 id="给用户推荐标签"><a href="#给用户推荐标签" class="headerlink" title="给用户推荐标签"></a>给用户推荐标签</h2><p>另外一种可行的推荐策略是不直接给用户推荐标的物，而是给用户推荐标签，用户通过关注推荐的标签，自动获取具备该标签的标的物。除了可以通过推荐的标签关联到标的物获得直接推荐标的物类似的效果外，间接的通过用户对推荐的标签选择、关注等行为进一步获得了用户的兴趣偏好，这是一种更简简单可行的推荐产品实现方案。</p><h1 id="基于内容的推荐算法的优缺点"><a href="#基于内容的推荐算法的优缺点" class="headerlink" title="基于内容的推荐算法的优缺点"></a>基于内容的推荐算法的优缺点</h1><h2 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h2><ol><li><p>可以很好的识别用户偏好<br>基于内容的推荐算法完全基于用户的历史兴趣来做推荐，推荐的标的物也跟用户的历史兴趣相似，所以推荐内容更加符合用户偏好。</p></li><li><p>直观易懂，解释性强<br>算法基于用户的兴趣为其推荐相似标的物，原理简单、容易理解。同时，由于是基于用户历史兴趣推荐跟兴趣相似的标的物，用户也非常容易接受和认可。</p></li><li><p>更加容易的解决冷启动<br>只要用户有一个系统操作系统，就可以基于内容为用户做推荐，随着行为加深，逐渐优化。同时对于新入库的标的物，只要它具备metadata信息等标的物相关信息，就可以利用基于内容的推荐算法将它分发出去。因此，对于强依赖于UGC内容的产品（抖音等），基于内容的推荐可以更好的对标的物提供方进行流量扶持。</p></li><li><p>算法实现相对简单<br>基于内容的推荐可以基于标签维度做推荐，也可以将标的物嵌入向量空间中，利用相似度做推荐，不管哪种方式，算法实现简单，有现成的开源算法库，容易落地。</p></li><li><p>对于小众领域也能有较好的推荐效果<br>对于冷门小众标的物，用户行为少，协同过滤等方法很难将这类内容分发出去，而基于内容的推荐算法受影响较小。</p></li><li><p>适合标的物快速增长的有时效性要求的产品<br>对于标的物增长很快的产品，如今日头条等新闻类，每天都有海量标的物入库，实效性也很强。新的标的物一般用户行为少，协同过滤算法很难将这些大量实时产生的新的标的物推荐出去，这时就可以采用基于内容的推荐算法更好的分发这些内容</p></li></ol><h2 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h2><ol><li><p>推荐范围狭窄，新颖性不强<br>由于该类算法只依赖于单个用户的行为为用户做推荐，推荐的结果会聚集在用户过去感兴趣的标的物类别上，如果用户不主动关注其他类型的标的物，很难为用户推荐多样性的结果，也无法挖掘用户深层次的潜在兴趣。特别对于新用户，只有少量行为，为用户推荐的标的物会比较单一。</p></li><li><p>需要知道相关的内容信息且处理起来较难<br>内容信息主要是文本、视频、音频，处理起来费力，相对难度较大，依赖领域知识。同时这些信息大概率含有噪音，增加处理难度。另外，对内容理解的全面性、完整性及准确性会影响推荐的效果。</p></li><li><p>较难将长尾标的物分发出去<br>基于内容的推荐需要用户对标的物有操作行为，长尾标的物一般操作行为少。由于基于内容的推荐只利用单个用户行为做推荐，所以更难将它分发给更多的用户。</p></li><li><p>推荐精准度不太高<br>相比协同过滤算法，基于内容的推荐算法精准度要差一些。</p></li></ol><h1 id="基于内容的推荐算法落地需要关注的重要问题"><a href="#基于内容的推荐算法落地需要关注的重要问题" class="headerlink" title="基于内容的推荐算法落地需要关注的重要问题"></a>基于内容的推荐算法落地需要关注的重要问题</h1><p>基于内容的推荐算法虽然容易理解、实现相对简单，但落地过程中，总结出来如下关键问题需要提前思考。</p><h2 id="内容来源的获取"><a href="#内容来源的获取" class="headerlink" title="内容来源的获取"></a>内容来源的获取</h2><ol><li><p>标的物“自身携带”的信息<br>标的物在上架时，第三方会准备相关的内容信息，如天猫上的商品在上架时会补充很多必要的信息。对于音视频，各类metadata信息也是入库上架时需要填充的信息。我们要做的是增加对新标的物入库的监控和审核，及时发现信息不全的情况并做适当处理。</p></li><li><p>通过爬虫获取标的物相关信息<br>通过爬虫爬取的信息可以作为标的物信息的补充，增加标的物特征完整表示能力的数据来源。</p></li><li><p>通过人工标注数据<br>往往人工标注的数据价值密度高，通过人工精准的标注可以大大提升算法推荐的精准度。问题是成本高。</p></li><li><p>通过运营活动或者产品交互让用户填的内容<br>通过抽奖活动让用户填写家庭组成、兴趣偏好等，在用户开始注册时让用户填写兴趣偏好特征，这些都是获取内容的手段。</p></li><li><p>通过收集用户行为直接获得或者预测推断出的内容<br>通过请求用户GPS位置知道用户的活动轨迹，用户购买时填写收货地址，用户绑定的身份证和银行卡等，通过用户操作行为预测出用户的兴趣偏好，这些方法都可以获得部分用户数据。</p></li><li><p>通过与第三方合作或者产品矩阵之间补充信息<br>目前中国有大数据交易市场，通过正规的数据交易或者跟其他公司合作，在不侵犯用户隐私的情况下，通过交换数据可以有效填补自己产品上缺失的数据。<br>如果公司有多个产品，新产品可以借助老产品的巨大用户基数，将新产品的用户与老产品用户关联起来(id-maping或者账号打通)，这样老产品上丰富的用户行为信息可以赋能给新产品。</p></li></ol><h2 id="利用负面反馈"><a href="#利用负面反馈" class="headerlink" title="利用负面反馈"></a>利用负面反馈</h2><p>用户对标的物的操作行为不一定代表正向反馈，有可能是负向的。比如点开一个视频，看了不到几秒就退出来了，明显表明用户不喜欢。有很多产品会在用户交互中直接提供负向反馈能力，这样可以收集到更多负向反馈。下面是今日头条和百度APP推荐的文章，右下角有一个小叉叉(见下面图9中红色圈圈)，点击后展示上面的白色交互区域，读者可以勾选几类不同的负向反馈机制。<br><img src="alg_negative.jpeg" alt="负面反馈形式"></p><p>负面反馈一般代表用户强烈的不满，因此利用好负反馈信息可以大大提升推荐系统的精准度和满意度。基于内容的推荐算法整合负反馈方式一般有以下几种：</p><ol><li><p>将负向反馈整合到算法模型中<br>在构建算法模型中整合负向反馈，跟正向反馈一起学习，从而更自然的整合负向反馈信息。</p></li><li><p>采用事后过滤方式<br>先生成推荐列表，再从该列表中过滤掉负向反馈关联或者相似的标的物。</p></li></ol><h2 id="兴趣随时间变化"><a href="#兴趣随时间变化" class="headerlink" title="兴趣随时间变化"></a>兴趣随时间变化</h2><p>用户的兴趣不是一成不变的，算法如何整合用户的兴趣变化？可行的策略是对用户的兴趣根据时间衰减，将用户操作行为投射到时间线上，并根据时间线的先后顺序给予不同权重，同时给用户建立短期兴趣特征和长期（一般）兴趣特征，在推荐时既考虑短期有考虑长期兴趣，最终推荐列表中整合两部分结果呈现。</p><h2 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h2><p>基于内容的推荐算法依赖于标的物相关的描述信息，这些信息更多的是以文本的形式存在，这就涉及到自然语言处理，文本中可能会存在很多歧义、符号、脏数据，我们需要实现对数据进行很好的处理，才能让后续的推荐算法产生好的效果。</p><h2 id="加速计算与节省资源"><a href="#加速计算与节省资源" class="headerlink" title="加速计算与节省资源"></a>加速计算与节省资源</h2><p>在实际推荐算法落地时，我们会事先为每个标的物计算N(=50)个最相似的标的物，事先将计算好的标的物存起来，减少时间和空间成本，方便后续更好地做推荐。同时也可以利用各种分布式计算平台和快速查询平台(如Spark、FAISS库等)加速计算过程。另外，算法开发过程中尽量做到模块化，对业务做抽象封装，这可以大大提升开发效率，并且可能会节省很多资源。</p><h2 id="解决基于内容的推荐越推越窄的问题"><a href="#解决基于内容的推荐越推越窄的问题" class="headerlink" title="解决基于内容的推荐越推越窄的问题"></a>解决基于内容的推荐越推越窄的问题</h2><p>前面提到基于内容的推荐存在越推越窄的缺点，那怎么避免或者减弱这种影响呢？当然用协同过滤等其他算法是一个有效的方法。另外，我们可以给用户做兴趣探索，为用户推荐兴趣之外的特征关联的标的物，通过用户的反馈来拓展用户兴趣空间，这类方法就是强化学习中的EE方法。如果我们构造了标的物的知识图谱系统，我们就可以通过图谱拓展标的物更远的联系，通过长线的相关性来做推荐，同样可以有效解决越推越窄的问题。</p><h2 id="工程落地技术选型"><a href="#工程落地技术选型" class="headerlink" title="工程落地技术选型"></a>工程落地技术选型</h2><p>本篇文章主要讲的是基于内容的推荐系统的算法实现原理，具体工程实践时，需要考虑到数据处理、模型训练、分布式计算等技术，当前很多开源方案可以使用，常用的如Spark mllib，scikit-learn，Tensorflow，pytorch，gensim等，这些工具都封装了很多数据处理、特征提取、机器学习算法，我们可以基于第二节的算法思路来落地实现。</p><h2 id="业务的安全性"><a href="#业务的安全性" class="headerlink" title="业务的安全性"></a>业务的安全性</h2><p>除了技术外，在推荐产品落地中还需要考虑推荐的标的物的安全性，避免推荐反动、色情、标题党、低俗内容，这些就需要基于NLP或者CV技术对文本或者视频进行分析过滤。如果是UGC平台型的产品，还需要考虑怎么激励优质内容创作者，让好的内容得到更多的分发机会，同时对产生劣质内容的创作者采取一定的惩罚措施，比如限制发文频率、禁止一段时间的发文权限等。</p><p>转载自 <a href="https://blog.csdn.net/qq_43045873/article/details/93816108" target="_blank" rel="noopener">基于内容的推荐算法</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 内容推荐 </tag>
            
            <tag> 向量化表示 </tag>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>理解“无穷小”</title>
      <link href="/2021/10/25/about-infinitesimal/"/>
      <url>/2021/10/25/about-infinitesimal/</url>
      
        <content type="html"><![CDATA[<blockquote><p>九城爱数学</p></blockquote><h1 id="什么是无穷小？"><a href="#什么是无穷小？" class="headerlink" title="什么是无穷小？"></a>什么是无穷小？</h1><p>无穷小数学符号记为 $\epsilon$ , 它的数学性质应该满足：<br><strong>对于任意给定实数 $\gamma$&gt;0</strong>，都满足 0&lt;|$\epsilon$|&lt;$\gamma$。很明显，无穷小是数学概念，并不可能存在于实数域中。<br>以极限形式定义会得到更加简洁直观的表达：满足$\lim\limits_{x\to c}f(x) = 0$ 的 $f(x)$ 是 $x\to c$ 时的无穷小。</p><p>无穷小有一个重要的隐含性质，即参考系无关，类似物理定律，在任意参照物系统里面，都绝对成立。<br>那么无穷小到底是一个数么？以反证法来说明，如果无穷小是一个数，那么这个数一定不会小于它本身，则违反了上述无穷小定义公式，所以无穷小并不是一个数。<br>实际上，在经典的极限和微积分定义中，无穷小通常是以函数、数列形式表达，可以自由表达。</p><h1 id="关于0-and-无穷小"><a href="#关于0-and-无穷小" class="headerlink" title="关于0 and 无穷小"></a>关于0 and 无穷小</h1><p>0，你可以看成是常数，也可以是常数函数 $f(x)=0$ 或者常数数列 $ ${$0, 0, 0, …$}$ $，都符合无穷小的定义，绝对值小于任意正实数。<br>无穷小与0一个最重要的区别是数学上，无穷小可以用作除法，两个相关（相关的意义是具有函数关系）的无穷小的比值就是这个函数的导数。</p><h1 id="无穷小-and-物理"><a href="#无穷小-and-物理" class="headerlink" title="无穷小 and 物理"></a>无穷小 and 物理</h1><p>数学意义上的无穷小，是一个混乱且容易引起歧义的量，正在逐渐消失在视野；但是在屋里中，这个无穷小确是切实存在的。<br>物理学的意义是用数学描述现实世界的科学，一个重要区别是数学本质是连续的，数学上的离散化是在连续基础上界定和设计出来，而物理学对现实世界的描述过程中，引发了多次革命，最为重要的一次革命就是现实世界的离散化（量子化），而这个离散化后的常量值就是物理世界中的无穷小，即普朗克常数。</p><p>** 刚开始逼着写博客，有点累，休息了。实际上未完。。。 **</p>]]></content>
      
      
      <categories>
          
          <category> 数学 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数学 </tag>
            
            <tag> 极限 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于EIP-1559协议</title>
      <link href="/2021/10/24/eip1559/"/>
      <url>/2021/10/24/eip1559/</url>
      
        <content type="html"><![CDATA[<h1 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h1><p>比特币以一种全新的思路重建了货币体系，这个传统情况下被认为一定需要权威中心（政府）背书和管理的核心金融基础设施，以太坊则带来了由加密货币到去中心化应用生态爆发，从ICO到Defi，再到NFT，区块链已经越来越接近真实生活。秉承去中心化理念的全面应用，以太坊升级也是去中心化的，任何一个feature改进或升级都需要参与者共同确认，因为EIP（以太坊改进提案）应运而生，而EIP1559则可视为以太坊最重要的提案之一，因为它的目的是改革ETH的Gas费定价机制，让用户以合理可接受的成本使用以太坊。</p><h1 id="以太坊提案EIP-1559"><a href="#以太坊提案EIP-1559" class="headerlink" title="以太坊提案EIP-1559"></a>以太坊提案EIP-1559</h1><p>EIP-1559是以太坊的重要提案，它对未来ETH的价值捕获、用户体验、安全性等方面都会产生重大影响。中短期内，EIP-1559提案对ETH的价格冲击力可能仅次于ETH的PoS，从长期看，可能会有更持久的影响力。</p><h2 id="什么是EIP-1559？"><a href="#什么是EIP-1559？" class="headerlink" title="什么是EIP-1559？"></a>什么是EIP-1559？</h2><p>EIP-1559本质上是关于以太坊网络交易定价机制的解决方案，它包括每区块网络费用的固定部分base fee（也就是基础费用，会被销毁，矿工收不到这笔费用），同时还有动态的可伸缩的区块大小设计，以应对瞬时的网络拥堵。</p><p>虽然每区块的基础费用是固定的，但是它会根据网络的拥堵情况，调节每区块的基础费用，它有一个公式用来调整基础费用的上升或下降。它会根据上一个区块所用的gas和目标gas（gas target，也就是之前的gas limit）来调整。当区块高于目标gas价格，基础费用上升，当区块低于目标gas价格，基础费用下降。</p><p>除了基础费用，还可以有打赏费用（小费）。在拥堵时，小费用于激励矿工将用户交易打包进区块。交易可以指定基础费用和小费的上限。这一提案还包括过渡性方案。开始时，区块的一半保留原来的竞价机制，一半采用新的费用机制，并逐渐过渡到新的方案。</p><p>这里有几个关键点：</p><p>费用结构</p><p>基本费用+小费</p><p>费用流向</p><p>基本费用会被销毁，矿工无法获得；矿工的收益主要是新增区块奖励+小费</p><p>弹性区块</p><p>可伸缩区块用以应对网络拥堵，同时基本费用也根据拥堵情况进行调整。</p><p><img src="eip1559.jpeg" alt="eip1559提案"></p><h2 id="为什么要搞EIP-1559？"><a href="#为什么要搞EIP-1559？" class="headerlink" title="为什么要搞EIP-1559？"></a>为什么要搞EIP-1559？</h2><p>对发生在以太坊上的交易，目前的收费方案是拍卖机制。由用户出价，矿工选择出价最高的交易，将其打包进区块。这种方式看上去简单且高效，不过也有问题：</p><p>竞价的低效率</p><p>以太坊上的最低出价可能会存在极大的差异。这导致出价效率低下。用户发送交易时选择出价的费用上限，而矿工选择最高出价的交易。对用户来说，出价多少合适是很难预估的，即便有复杂的费用预估算法，也不太可能做到很好的估算，经常会出现超额费用支付的情况。</p><p>造成延误</p><p>由于每区块gas limit的限制和交易量的自然波动，这会导致交易可能需要等待几个区块才能被打包进入。对用户来说，可能会造成延误。而EIP-1559提出弹性区块的机制，可以让一个区块变大，而下一个区块变小，实现更长期的平均区块大小限制，允许不同区块存在一定的大小差异。这解决了当前一些区块过满，而一些区块使用过少的现状。</p><p>安全性</p><p>随着区块奖励的减少（如比特币和Zcash），未来交易费成为奖励矿工的主要来源。因此需要有足够的交易规模来支撑网络的安全，同时还可能会导致矿工的自私挖矿等问题，产生一些不稳定因素。</p><p>EIP-1559为了解决上述问题，它改变了付费结构和付费流向：将交易费用分为基本费用+小费，同时基本费用被销毁，矿工无法获得，矿工可以获得打赏费用，也可以获得新增发的区块奖励，从而减少矿工操纵交易费用的动机，并让包括矿工在内各种参与者有机会获益。</p><p>此外，在EIP-1559的设计中，当网络超出每区块目标gas使用量，基本费用会在接下来的区块增加，而当容量低于目标gas使用量，则下降。因此，gas费用的变化是根据需求情况进行调整的，其区块之间的费用差异是可预测的。</p><p>这对于用户体验来说会有提升。EIP-1559实施之前的以太坊采用的是类似于比特币的第一价格拍卖机制（First Price Auctions），其缺点之一是需要进行费用估算。而EIP-1559则对所有交易尽可能实施相同的费率。用户需要决定的是是否支付费用，而不用太多考虑出价多少。钱包可以帮助用户预测基本费用以及自动设置一点打赏费用，减少了大多数用户手动调整gas费用的操作。当然，如果用户手动设置交易费用上限也是可以的,用户通过手动设置费用上限可以限制其最高成本。</p><p>除了用户体验提升，还可以提升以太坊的安全性以及解决经济抽象等问题。</p><p>由于矿工在交易费用中仅仅捕获打赏费用，而基本费用被协议销毁，这样可以消除矿工操作费用以获得更多费用的动机。更重要的是，它改变了ETH的价值捕获机制。之前的ETH更多代表了以太坊上价值流通和价值存储，本身并没有捕获费用价值。</p><p>而基本费用改变了这一局面。基本费用的捕获意味着ETH开始捕获以太坊网络的交易费用，而交易费用的多少取决于以太坊上交易规模。随着以太坊交易规模的增大，ETH可以捕获的费用就越多。</p><p>同时，在EIP-1559实施前的以太坊交易费用支付中，并不是一定得用ETH支付。而实施之后基本费用是使用ETH支付，这也在一定程度上解决了之前讨论的“经济抽象”的问题。所谓的“经济抽象”，就是说用户可以使用稳定币或其他代币支付手续费。这不仅会威胁原生代币ETH在网络中的位置，而且也不利于网络安全。EIP-1559要求消耗特定数量的ETH，会增加经济抽象的难度。</p><p>如果以太坊上的交易规模足够大，那么它可以捕获大量的基本费用，这些用ETH支付的基本费用会被销毁，这会降低ETH的通胀率，甚至可能通缩的情况（假设增发低于销毁）。这有利于ETH的价值支撑。</p><p>从安全性来看，在比特币的激励中，当区块补贴完毕，它主要依靠交易费用来维持其安全，这种模式如何获得可持续目前还是存在争议的。而以太坊通过将交易手续费销毁，并将矿工激励通过区块奖励持续运作下去。这是一个更为弹性的机制。由于交易手续费销毁，价值转移到ETH上，以太坊采用ETH区块奖励对矿工进行补贴，这为以太坊安全的长期可持续提供了基础。</p><h2 id="EIP-1559对ETH价值的支撑"><a href="#EIP-1559对ETH价值的支撑" class="headerlink" title="EIP-1559对ETH价值的支撑"></a>EIP-1559对ETH价值的支撑</h2><p>通过过去一段时间的实践，我们可以看到单纯挖矿代币的价值支撑存在很大的问题。有些挖矿奖励代币，或者说是所谓的“治理代币”，有很大的抛压，但价值支撑不够，最终导致币价下跌，激励下降，甚至出现了不少死亡螺旋的代币。</p><p>有些头部DEX交易量很好，流动性也不错，协议的年化手续费用也非常高，基本面一片大好，但同样也无法支撑起价值。因为它们也同样面临“挖卖提”的情况。其代币本身只是“治理代币”，没有捕获协议价值。目前的协议费用中，绝大部分收益依然没有融入到代币中来，而是直接分给LP。这意味着代币和协议的价值成长并没有达成高度融合。这就是在当前情况下它们的价值无法支撑的原因所在。当然，未来这种局面可能会随着治理的深入发生改变，因为治理本身可以改变这个局面。目前的治理代币更多是代表了未来的可能性。</p><p>EIP-1559对ETH的影响也是类似的。EIP-1559实施之前的ETH并没有捕获到以太坊网络费用的价值，这导致ETH在泡沫破灭时完全无法实现价值的支撑，从泡沫时最高的近1500美元跌至底点的100多美元。</p><p>而EIP-1559之前的ETH将捕获以太坊网络的费用价值，截止到蓝狐笔记写稿时，以太坊的年化网络费用超过了13亿美元，这是一笔不小的费用。</p><p>这意味着，未来ETH除了作为价值存储和价值媒介之外，还可以真正捕获到大规模的价值。就其当前价值捕获的规模看，它是目前所有公链中最强势的，这让ETH在未来有更坚实的价值支撑。</p>]]></content>
      
      
      <categories>
          
          <category> 区块链 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 以太坊 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>enjoy new world</title>
      <link href="/2021/10/15/enjoy-new-world/"/>
      <url>/2021/10/15/enjoy-new-world/</url>
      
        <content type="html"><![CDATA[<blockquote><p>关于写博客</p></blockquote><p><img src="/medias/contact.jpg" alt></p><h2 id="为什么要创建自己的博客站"><a href="#为什么要创建自己的博客站" class="headerlink" title="为什么要创建自己的博客站"></a>为什么要创建自己的博客站</h2><p><strong><em>曾经对博客不屑，但到了该记录的年纪</em></strong></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
